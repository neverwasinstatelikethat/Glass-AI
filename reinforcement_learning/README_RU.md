# Модуль обучения с подкреплением Glass AI

## Обзор

Модуль обучения с подкреплением (RL) системы Glass AI реализует автономную оптимизацию производственных параметров с использованием алгоритма Proximal Policy Optimization (PPO). Этот модуль анализирует прогнозы ИИ и предоставляет рекомендации по корректировке параметров производства для минимизации дефектов и максимизации эффективности.

## Архитектура

```
Обучение с подкреплением
├── PPO оптимизатор
│   ├── Агент PPO
│   ├── Политика
│   ├── Критик
│   └── Буфер опыта
├── Тренировочный конвейер RL
│   ├── Среда обучения
│   ├── Наградная функция
│   ├── Симулятор цифрового двойника
│   └── Мониторинг обучения
└── Чекпоинты обучения
    ├── Последние веса модели
    ├── Статистика обучения
    └── История эпизодов
```

## Основные компоненты

### 1. PPO оптимизатор (`ppo_optimizer.py`)
Реализация алгоритма Proximal Policy Optimization для оптимизации производственных параметров.

#### Особенности:
- **Агент PPO**: Реализация алгоритма PPO с clipped surrogate objective
- **Политика**: Стохастическая политика для выбора действий
- **Критик**: Оценка функции ценности состояний
- **Буфер опыта**: Хранение переходов для обучения на опыте
- **Clipped Surrogate**: Защита от разрушительных обновлений политики

### 2. Тренировочный конвейер RL (`rl_training_pipeline.py`)
Конвейер для обучения агента RL в симулированной среде цифрового двойника.

#### Особенности:
- **Среда обучения**: Среда Gym для обучения агента
- **Наградная функция**: Функция награды, основанная на качестве продукции
- **Симулятор цифрового двойника**: Интеграция с цифровым двойником для симуляции
- **Мониторинг обучения**: Отслеживание прогресса обучения и метрик

## Алгоритм PPO

### Принцип работы:
1. **Сбор опыта**: Агент взаимодействует со средой и собирает переходы
2. **Обновление политики**: Оптимизация политики с использованием clipped surrogate
3. **Обновление критика**: Обучение функции ценности
4. **Итерация**: Повторение процесса до сходимости

### Преимущества PPO:
- **Стабильность**: Более стабильное обучение по сравнению с другими алгоритмами
- **Простота настройки**: Меньше гиперпараметров для настройки
- **Эффективность образцов**: Хорошее использование собранных данных
- **Защита от разрушительных обновлений**: Clipped surrogate предотвращает большие обновления

## Среда обучения

### Наблюдения (состояния):
- Температура печи
- Скорость формовочной линии
- Температура формы
- Давление формования
- Скорость охлаждения
- Текущие прогнозы дефектов
- Исторические данные параметров

### Действия:
- Изменение температуры печи (-50°C до +50°C)
- Изменение скорости формовочной линии (-20% до +20%)
- Изменение температуры формы (-50°C до +50°C)
- Изменение давления формования (-10 МПа до +10 МПа)
- Изменение скорости охлаждения (-2°C/мин до +2°C/мин)

### Наградная функция:
```
Награда = -дефекты * 10 + производительность * 0.1 - энергопотребление * 0.01
```

#### Компоненты награды:
- **Дефекты**: Штраф за предсказанные дефекты (-10 за единицу)
- **Производительность**: Поощрение за высокую производительность (+0.1 за единицу)
- **Энергопотребление**: Штраф за высокое энергопотребление (-0.01 за единицу)

## Цифровой двойник

### Интеграция:
- Симуляция производственного процесса
- Прогнозирование результатов действий
- Безопасное обучение без риска для реального производства

### Физические модели:
- Тепловая динамика печи
- Модель вязкости стекла
- Анализ напряжений при охлаждении

## Обучение

### Процесс обучения:
1. **Инициализация**: Загрузка начальных весов или случайная инициализация
2. **Сбор эпизодов**: Агент взаимодействует со средой
3. **Обновление**: Оптимизация политики и критика
4. **Оценка**: Тестирование политики на новых эпизодах
5. **Итерация**: Повторение до достижения целевых метрик

### Гиперпараметры:
- **Learning rate**: 3e-4
- **Discount factor (γ)**: 0.99
- **Clipping parameter (ε)**: 0.2
- **Epochs per update**: 10
- **Batch size**: 64
- **Horizon**: 2048 шагов

## Безопасное исследование

### Механизмы безопасности:
- **Ограничения действий**: Действия ограничены безопасными диапазонами
- **Проверка допустимости**: Проверка действий на соответствие технологическим ограничениям
- **Откат при аварии**: Возможность отката к предыдущей политике при катастрофическом поведении

### Уровни автономии:
1. **Ручной режим**: Все действия выполняются оператором
2. **Режим с одобрением**: Рекомендации RL требуют одобрения оператора
3. **Автономный режим**: RL агент принимает решения самостоятельно

## Рекомендации

### Генерация рекомендаций:
- Анализ текущего состояния производства
- Прогнозирование результатов потенциальных действий
- Ранжирование действий по ожидаемой награде
- Оценка уверенности в рекомендациях

### Формат рекомендаций:
```json
{
  "timestamp": "2025-12-05T10:00:00Z",
  "recommendations": [
    {
      "action": "adjust_furnace_temperature",
      "value": 1550.0,
      "confidence": 0.92,
      "expected_reward": 15.3,
      "risk_level": "LOW"
    },
    {
      "action": "reduce_belt_speed",
      "value": 145.0,
      "confidence": 0.87,
      "expected_reward": 12.1,
      "risk_level": "MEDIUM"
    }
  ]
}
```

## Мониторинг обучения

### Метрики:
- **Средняя награда за эпизод**: Общая производительность агента
- **Длина эпизода**: Количество шагов до завершения
- **Потери политики**: Ошибка обновления политики
- **Потери критика**: Ошибка оценки функции ценности
- **Коэффициент clipping**: Доля clipped обновлений

### Визуализация:
- Кривые обучения
- Анализ политики
- Распределение действий
- Эволюция наград

## Чекпоинты

### Структура чекпоинтов:
- **Веса модели**: Сохраненные веса политики и критика
- **Статистика обучения**: Метрики и гиперпараметры
- **История эпизодов**: Записи о пройденных эпизодах
- **Конфигурация**: Параметры обучения

### Управление чекпоинтами:
- Автоматическое сохранение каждые N эпизодов
- Сохранение лучших моделей по метрикам
- Возможность загрузки предыдущих версий

## Интеграция с другими модулями

### С прогнозами ИИ:
- Получение прогнозов дефектов от моделей LSTM, ViT, GNN
- Использование прогнозов в наградной функции
- Интеграция неопределенности прогнозов

### С цифровым двойником:
- Симуляция действий в цифровом двойнике
- Получение результатов симуляций
- Обновление моделей на основе реальных данных

### С бэкендом:
- Предоставление рекомендаций через API
- Получение данных о текущем состоянии производства
- Интеграция с системой автономных действий

## Производительность

### Время вывода:
- **Генерация рекомендаций**: ~15 мс
- **Оценка действий**: ~5 мс
- **Полная итерация**: ~25 мс

### Использование ресурсов:
- **CPU**: 2-4 ядра при выводе
- **RAM**: 2-4 ГБ
- **GPU**: Не требуется для вывода (опционально для обучения)

## Обучение моделей

### Требования к обучению:
- **GPU**: Рекомендуется для ускорения обучения
- **Время обучения**: 2-4 часа для начального обучения
- **Данные**: Синтетические данные из цифрового двойника

### Запуск обучения:
```bash
python reinforcement_learning/rl_training_pipeline.py
```

### Мониторинг обучения:
```bash
tensorboard --logdir=reinforcement_learning/logs
```

## Тестирование

### Юнит-тесты:
```bash
pytest tests/reinforcement_learning/
```

### Интеграционные тесты:
- Тестирование взаимодействия с цифровым двойником
- Тестирование генерации рекомендаций
- Тестирование безопасных действий

### Тесты производительности:
- Измерение времени вывода
- Тестирование масштабирования
- Стресс-тестирование

## Развертывание

### Локальное развертывание:
```bash
python -m reinforcement_learning.ppo_optimizer
```

### Docker развертывание:
```dockerfile
FROM python:3.8-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY reinforcement_learning/ reinforcement_learning/

CMD ["python", "-m", "reinforcement_learning.ppo_optimizer"]
```

### Kubernetes развертывание:
- Деплоймент для масштабирования
- Сервисы для внутренней коммуникации
- Persistent volumes для чекпоинтов

## Безопасность

### Аутентификация:
- JWT токены для API
- Проверка прав доступа к RL рекомендациям
- Аудит действий RL агента

### Защита от атак:
- Ограничение частоты запросов
- Валидация входных данных
- Логирование подозрительных действий

## Устранение неполадок

### Распространенные проблемы:

1. **Низкая награда агента**:
   - Проверьте наградную функцию
   - Убедитесь, что среда правильно симулирует производство
   - Попробуйте другие гиперпараметры

2. **Нестабильное обучение**:
   - Уменьшите learning rate
   - Увеличьте batch size
   - Проверьте clipping parameter

3. **Плохие рекомендации**:
   - Проверьте интеграцию с моделями ИИ
   - Убедитесь, что прогнозы корректны
   - Проверьте ограничения действий

## Версионирование

### Управление версиями:
- SemVer для версионирования моделей
- Git tags для релизов
- Changelog для изменений

### Откат версий:
- Возможность загрузки предыдущих версий
- Сравнение производительности версий
- A/B тестирование политик

## Логирование

### Уровни логирования:
- **DEBUG**: Подробная информация для диагностики
- **INFO**: Общая информация о работе агента
- **WARNING**: Предупреждения о потенциальных проблемах
- **ERROR**: Ошибки, требующие внимания
- **CRITICAL**: Критические ошибки, требующие немедленного вмешательства

### Структура логов:
- Временные метки
- Уровень логирования
- Идентификаторы сессий
- Структурированные данные