"""
Edge inference pipeline –¥–ª—è –∑–∞–ø—É—Å–∫–∞ –º–æ–¥–µ–ª–µ–π –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞—Ö NVIDIA Jetson
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç ONNX/TensorRT –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é –∏ offline —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, List, Tuple, Optional, Union, Any
import onnx
import onnxruntime as ort
import logging
from datetime import datetime
import json
import time

# –î–ª—è TensorRT (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    import tensorrt as trt
    import pycuda.driver as cuda
    import pycuda.autoinit
    TENSORRT_AVAILABLE = True
except ImportError:
    TENSORRT_AVAILABLE = False
    logging.warning("TensorRT –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω ONNX Runtime")

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ONNXModelRunner:
    """Runner –¥–ª—è ONNX –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, model_path: str, providers: List[str] = None):
        """
        Args:
            model_path: –ø—É—Ç—å –∫ ONNX –º–æ–¥–µ–ª–∏
            providers: —Å–ø–∏—Å–æ–∫ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –¥–ª—è ONNX Runtime
        """
        self.model_path = model_path
        
        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø—Ä–æ–≤–∞–π–¥–µ—Ä–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        if providers is None:
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
        
        # –°–æ–∑–¥–∞–Ω–∏–µ inference —Å–µ—Å—Å–∏–∏
        self.session = ort.InferenceSession(model_path, providers=providers)
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω—ã—Ö –Ω–∞–∑–≤–∞–Ω–∏–π
        self.input_names = [input.name for input in self.session.get_inputs()]
        self.output_names = [output.name for output in self.session.get_outputs()]
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ ONNX –º–æ–¥–µ–ª—å: {model_path}")
        logger.info(f"üì• –í—Ö–æ–¥—ã: {self.input_names}")
        logger.info(f"üì§ –í—ã—Ö–æ–¥—ã: {self.output_names}")
    
    def predict(self, inputs: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é ONNX –º–æ–¥–µ–ª–∏
        
        Args:
            inputs: —Å–ª–æ–≤–∞—Ä—å —Å –≤—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ {input_name: data}
            
        Returns:
            outputs: —Å–ª–æ–≤–∞—Ä—å —Å –≤—ã—Ö–æ–¥–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ {output_name: data}
        """
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≤—Ö–æ–¥–æ–≤
        ort_inputs = {
            name: inputs[name] for name in self.input_names
        }
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        start_time = time.time()
        ort_outputs = self.session.run(self.output_names, ort_inputs)
        inference_time = time.time() - start_time
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        outputs = {
            name: output for name, output in zip(self.output_names, ort_outputs)
        }
        
        logger.debug(f"‚è±Ô∏è –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞: {inference_time:.4f} —Å–µ–∫")
        
        return outputs


class TensorRTModelRunner:
    """Runner –¥–ª—è TensorRT –º–æ–¥–µ–ª–µ–π (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)"""
    
    def __init__(self, engine_path: str):
        """
        Args:
            engine_path: –ø—É—Ç—å –∫ TensorRT engine —Ñ–∞–π–ª—É
        """
        if not TENSORRT_AVAILABLE:
            raise RuntimeError("TensorRT –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
        
        self.engine_path = engine_path
        self.engine = self._load_engine(engine_path)
        self.context = self.engine.create_execution_context()
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–æ–≤ –≤—Ö–æ–¥–æ–≤/–≤—ã—Ö–æ–¥–æ–≤
        self.input_shapes = []
        self.output_shapes = []
        
        for i in range(self.engine.num_bindings):
            if self.engine.binding_is_input(i):
                self.input_shapes.append(self.engine.get_binding_shape(i))
            else:
                self.output_shapes.append(self.engine.get_binding_shape(i))
        
        logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω TensorRT engine: {engine_path}")
    
    def _load_engine(self, engine_path: str):
        """–ó–∞–≥—Ä—É–∑–∫–∞ TensorRT engine"""
        with open(engine_path, "rb") as f:
            runtime = trt.Runtime(trt.Logger(trt.Logger.WARNING))
            engine = runtime.deserialize_cuda_engine(f.read())
        return engine
    
    def predict(self, inputs: List[np.ndarray]) -> List[np.ndarray]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é TensorRT
        
        Args:
            inputs: —Å–ø–∏—Å–æ–∫ –≤—Ö–æ–¥–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤
            
        Returns:
            outputs: —Å–ø–∏—Å–æ–∫ –≤—ã—Ö–æ–¥–Ω—ã—Ö –º–∞—Å—Å–∏–≤–æ–≤
        """
        # –í—ã–¥–µ–ª–µ–Ω–∏–µ GPU –ø–∞–º—è—Ç–∏
        bindings = []
        for i in range(self.engine.num_bindings):
            binding_shape = self.engine.get_binding_shape(i)
            size = trt.volume(binding_shape) * self.engine.max_batch_size * np.dtype(np.float32).itemsize
            bindings.append(cuda.mem_alloc(size))
        
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –≤ GPU
        for i, input_data in enumerate(inputs):
            cuda.memcpy_htod(bindings[i], input_data)
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
        start_time = time.time()
        self.context.execute_v2(bindings=bindings)
        inference_time = time.time() - start_time
        
        # –ö–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏–∑ GPU
        outputs = []
        for i in range(len(inputs), len(bindings)):
            output_shape = self.engine.get_binding_shape(i)
            output_size = trt.volume(output_shape) * self.engine.max_batch_size
            output_data = np.empty(output_size, dtype=np.float32)
            cuda.memcpy_dtoh(output_data, bindings[i])
            outputs.append(output_data.reshape(output_shape))
        
        logger.debug(f"‚è±Ô∏è –í—Ä–µ–º—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ (TensorRT): {inference_time:.4f} —Å–µ–∫")
        
        return outputs


class EdgeModelManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –º–æ–¥–µ–ª–µ–π –¥–ª—è edge —É—Å—Ç—Ä–æ–π—Å—Ç–≤"""
    
    def __init__(self, model_configs: Dict[str, Dict]):
        """
        Args:
            model_configs: –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π
                {
                    "lstm": {"path": "lstm_model.onnx", "type": "onnx"},
                    "vit": {"path": "vit_model.trt", "type": "tensorrt"},
                    ...
                }
        """
        self.model_configs = model_configs
        self.models = {}
        self.latency_stats = {}
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
        self._load_models()
    
    def _load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        for model_name, config in self.model_configs.items():
            try:
                if config["type"] == "onnx":
                    self.models[model_name] = ONNXModelRunner(config["path"])
                elif config["type"] == "tensorrt" and TENSORRT_AVAILABLE:
                    self.models[model_name] = TensorRTModelRunner(config["path"])
                else:
                    logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {config['type']}")
                
                self.latency_stats[model_name] = []
                logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                # Create a dummy model that returns zeros for predictions
                self.models[model_name] = None
    
    def predict(
        self, 
        model_name: str, 
        inputs: Union[Dict[str, np.ndarray], List[np.ndarray]]
    ) -> Union[Dict[str, np.ndarray], List[np.ndarray]]:
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é
        
        Args:
            model_name: –∏–º—è –º–æ–¥–µ–ª–∏
            inputs: –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
            
        Returns:
            outputs: –≤—ã—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        """
        if model_name not in self.models:
            raise ValueError(f"–ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        model = self.models[model_name]
        
        # Handle case where model failed to load
        if model is None:
            logger.warning(f"‚ö†Ô∏è –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞, –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            # Return zero arrays with appropriate shapes
            if isinstance(inputs, dict):
                # For ONNX models, return dict with same keys but zero arrays
                dummy_outputs = {}
                for key in inputs.keys():
                    dummy_outputs[key] = np.zeros((1, 6))  # Assuming 6 output classes
                return dummy_outputs
            else:
                # For other models, return list of zero arrays
                return [np.zeros((1, 6))]  # Assuming 6 output classes
        
        # –ò–∑–º–µ—Ä–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏
        start_time = time.time()
        
        if isinstance(model, ONNXModelRunner):
            outputs = model.predict(inputs)
        elif isinstance(model, TensorRTModelRunner):
            outputs = model.predict(inputs)
        else:
            raise RuntimeError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {type(model)}")
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ latency
        latency = time.time() - start_time
        self.latency_stats[model_name].append(latency)
        
        if len(self.latency_stats[model_name]) > 1000:
            self.latency_stats[model_name].pop(0)  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏—Å—Ç–æ—Ä–∏–∏
        
        return outputs
    
    def get_latency_stats(self, model_name: str) -> Dict[str, float]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ latency –¥–ª—è –º–æ–¥–µ–ª–∏"""
        if model_name not in self.latency_stats:
            return {}
        
        latencies = self.latency_stats[model_name]
        if not latencies:
            return {}
        
        return {
            "mean": np.mean(latencies),
            "std": np.std(latencies),
            "min": np.min(latencies),
            "max": np.max(latencies),
            "p95": np.percentile(latencies, 95),
            "p99": np.percentile(latencies, 99)
        }
    
    def get_all_latency_stats(self) -> Dict[str, Dict[str, float]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ latency –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        return {
            model_name: self.get_latency_stats(model_name)
            for model_name in self.latency_stats.keys()
        }
    
    def get_model_health(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ –º–æ–¥–µ–ª–µ–π"""
        health_info = {}
        
        for model_name, model in self.models.items():
            # –ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ latency
            latency_stats = self.get_latency_stats(model_name)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
            is_available = model is not None
            
            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ latency
            status = "healthy"
            if latency_stats:
                if latency_stats.get("p95", 0) > 0.1:  # 100ms threshold
                    status = "degraded"
                if latency_stats.get("p99", 0) > 0.5:  # 500ms threshold
                    status = "unhealthy"
            
            health_info[model_name] = {
                "status": status,
                "available": is_available,
                "latency_stats": latency_stats,
                "model_type": type(model).__name__,
                "last_updated": datetime.utcnow().isoformat()
            }
        
        return health_info
    
    def get_system_health(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—â–µ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–æ—Å—Ç–æ—è–Ω–∏–∏ —Å–∏—Å—Ç–µ–º—ã"""
        model_health = self.get_model_health()
        
        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç—É—Å–æ–≤ –º–æ–¥–µ–ª–µ–π
        healthy_count = sum(1 for info in model_health.values() if info["status"] == "healthy")
        degraded_count = sum(1 for info in model_health.values() if info["status"] == "degraded")
        unhealthy_count = sum(1 for info in model_health.values() if info["status"] == "unhealthy")
        
        # –û–±—â–∏–π —Å—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã
        if unhealthy_count > 0:
            system_status = "unhealthy"
        elif degraded_count > 0:
            system_status = "degraded"
        else:
            system_status = "healthy"
        
        return {
            "system_status": system_status,
            "total_models": len(model_health),
            "healthy_models": healthy_count,
            "degraded_models": degraded_count,
            "unhealthy_models": unhealthy_count,
            "model_health": model_health,
            "timestamp": datetime.utcnow().isoformat()
        }
    
    def get_model_info(self) -> Dict[str, Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""
        model_info = {}
        
        for model_name, config in self.model_configs.items():
            model_info[model_name] = {
                "config": config,
                "loaded": model_name in self.models,
                "latency_history_count": len(self.latency_stats.get(model_name, []))
            }
        
        return model_info


class MultiModelEnsembleInference:
    """–ê–Ω—Å–∞–º–±–ª–µ–≤—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    def __init__(self, model_manager: EdgeModelManager):
        self.model_manager = model_manager
        self.ensemble_weights = None
        self.model_performance = {}
    
    def predict_with_ensemble(
        self,
        model_inputs: Dict[str, Union[Dict[str, np.ndarray], List[np.ndarray]]],
        ensemble_weights: Optional[Dict[str, float]] = None
    ) -> Tuple[np.ndarray, Dict[str, np.ndarray]]:
        """
        –ê–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        
        Args:
            model_inputs: –≤—Ö–æ–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
            ensemble_weights: –≤–µ—Å–∞ –º–æ–¥–µ–ª–µ–π –≤ –∞–Ω—Å–∞–º–±–ª–µ
            
        Returns:
            ensemble_output: –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            individual_outputs: –∏–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –º–æ–¥–µ–ª–µ–π
        """
        individual_outputs = {}
        
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π
        for model_name, inputs in model_inputs.items():
            try:
                output = self.model_manager.predict(model_name, inputs)
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –ø–µ—Ä–≤—ã–π –≤—ã—Ö–æ–¥ - —ç—Ç–æ –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                if isinstance(output, dict):
                    main_output = list(output.values())[0]
                else:
                    main_output = output[0] if isinstance(output, list) else output
                individual_outputs[model_name] = main_output
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ {model_name}: {e}")
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω—É–ª–µ–≤–æ–π –º–∞—Å—Å–∏–≤ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                individual_outputs[model_name] = np.zeros(6)  # 6 –∫–ª–∞—Å—Å–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # –ê–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ
        if ensemble_weights is None:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –≤–µ—Å–∞ –∏–ª–∏ —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞
            if self.ensemble_weights is not None:
                ensemble_weights = self.ensemble_weights
            else:
                ensemble_weights = {
                    model_name: 1.0 / len(individual_outputs) 
                    for model_name in individual_outputs.keys()
                }
        else:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–µ –≤–µ—Å–∞
            self.ensemble_weights = ensemble_weights
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ—Å–æ–≤
        total_weight = sum(ensemble_weights.values())
        normalized_weights = {
            model_name: weight / total_weight 
            for model_name, weight in ensemble_weights.items()
        }
        
        # –í–∑–≤–µ—à–µ–Ω–Ω–æ–µ —Å—É–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ
        # Handle case where individual_outputs might be empty
        if not individual_outputs:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞—é—Ç—Å—è –Ω—É–ª–µ–≤—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è")
            return np.zeros(6), {}  # 6 –∫–ª–∞—Å—Å–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        
        # Get the shape from the first available output
        first_output = next(iter(individual_outputs.values()))
        ensemble_output = np.zeros_like(first_output)
        
        for model_name, weight in normalized_weights.items():
            if model_name in individual_outputs:
                ensemble_output += individual_outputs[model_name] * weight
        
        return ensemble_output, individual_outputs
    
    def update_ensemble_weights(self, model_performance: Dict[str, float]):
        """
        –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        
        Args:
            model_performance: —Å–ª–æ–≤–∞—Ä—å {model_name: performance_score}
        """
        self.model_performance = model_performance
        
        # –ü—Ä–æ—Å—Ç–æ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        total_performance = sum(model_performance.values())
        if total_performance > 0:
            self.ensemble_weights = {
                model_name: performance / total_performance
                for model_name, performance in model_performance.items()
            }
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–∞–≤–Ω–æ–º–µ—Ä–Ω—ã–µ –≤–µ—Å–∞
            self.ensemble_weights = {
                model_name: 1.0 / len(model_performance)
                for model_name in model_performance.keys()
            }
    
    def get_ensemble_weights(self) -> Optional[Dict[str, float]]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–∏—Ö –≤–µ—Å–æ–≤ –∞–Ω—Å–∞–º–±–ª—è
        
        Returns:
            ensemble_weights: —Å–ª–æ–≤–∞—Ä—å –≤–µ—Å–æ–≤ –º–æ–¥–µ–ª–µ–π
        """
        return self.ensemble_weights
    
    def get_model_performance(self) -> Dict[str, float]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        
        Returns:
            model_performance: —Å–ª–æ–≤–∞—Ä—å –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π
        """
        return self.model_performance


def convert_to_onnx(
    model: nn.Module,
    input_shape: Tuple[int, ...],
    output_path: str,
    opset_version: int = 11
):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è PyTorch –º–æ–¥–µ–ª–∏ –≤ ONNX"""
    model.eval()
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø—Ä–∏–º–µ—Ä–Ω–æ–≥–æ –≤—Ö–æ–¥–∞
    dummy_input = torch.randn(input_shape)
    
    # –≠–∫—Å–ø–æ—Ä—Ç –≤ ONNX
    torch.onnx.export(
        model,
        dummy_input,
        output_path,
        export_params=True,
        opset_version=opset_version,
        do_constant_folding=True,
        input_names=['input'],
        output_names=['output'],
        dynamic_axes={
            'input': {0: 'batch_size'},
            'output': {0: 'batch_size'}
        }
    )
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ ONNX: {output_path}")


def convert_to_tensorrt(
    onnx_path: str,
    engine_path: str,
    max_batch_size: int = 1,
    max_workspace_size: int = 1 << 30  # 1GB
):
    """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è ONNX –º–æ–¥–µ–ª–∏ –≤ TensorRT engine"""
    if not TENSORRT_AVAILABLE:
        logger.warning("TensorRT –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω")
        return
    
    # –°–æ–∑–¥–∞–Ω–∏–µ builder
    builder = trt.Builder(trt.Logger(trt.Logger.WARNING))
    network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))
    parser = trt.OnnxParser(network, builder.logger)
    
    # –ü–∞—Ä—Å–∏–Ω–≥ ONNX –º–æ–¥–µ–ª–∏
    with open(onnx_path, 'rb') as model_file:
        parser.parse(model_file.read())
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è builder
    config = builder.create_builder_config()
    config.max_workspace_size = max_workspace_size
    
    # –°–æ–∑–¥–∞–Ω–∏–µ engine
    engine = builder.build_engine(network, config)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ engine
    with open(engine_path, 'wb') as f:
        f.write(engine.serialize())
    
    logger.info(f"‚úÖ –ú–æ–¥–µ–ª—å —Å–∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ TensorRT: {engine_path}")


def create_edge_inference_pipeline(
    model_configs: Dict[str, Dict],
    ensemble_weights: Optional[Dict[str, float]] = None
) -> Tuple[EdgeModelManager, MultiModelEnsembleInference]:
    """–°–æ–∑–¥–∞–Ω–∏–µ edge inference pipeline"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –º–æ–¥–µ–ª–µ–π
    model_manager = EdgeModelManager(model_configs)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    ensemble_inference = MultiModelEnsembleInference(model_manager)
    
    logger.info("‚úÖ Edge inference pipeline —Å–æ–∑–¥–∞–Ω")
    
    return model_manager, ensemble_inference


# ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge inference pipeline
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ edge inference pipeline...")
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π –∏ –∏—Ö –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è
    print("  –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    
    # LSTM –º–æ–¥–µ–ª—å
    from models.lstm_predictor.attention_lstm import create_lstm_model
    lstm_model = create_lstm_model(input_size=10, hidden_size=32, num_layers=1, output_size=5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∫ ONNX
    convert_to_onnx(
        lstm_model,
        input_shape=(1, 30, 10),  # batch_size=1, seq_len=30, input_size=10
        output_path="test_lstm_model.onnx"
    )
    
    # ViT –º–æ–¥–µ–ª—å
    from models.vision_transformer.defect_detector import create_vit_classifier
    vit_model = create_vit_classifier(img_size=32, patch_size=8, n_classes=5)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–∞–∫ ONNX
    convert_to_onnx(
        vit_model,
        input_shape=(1, 3, 32, 32),  # batch_size=1, channels=3, height=32, width=32
        output_path="test_vit_model.onnx"
    )
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
    model_configs = {
        "lstm": {"path": "test_lstm_model.onnx", "type": "onnx"},
        "vit": {"path": "test_vit_model.onnx", "type": "onnx"}
    }
    
    # –°–æ–∑–¥–∞–Ω–∏–µ pipeline
    print("  –°–æ–∑–¥–∞–Ω–∏–µ inference pipeline...")
    model_manager, ensemble_inference = create_edge_inference_pipeline(model_configs)
    
    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    
    # LSTM –≤—Ö–æ–¥—ã
    lstm_input = {
        "input": np.random.randn(1, 30, 10).astype(np.float32)
    }
    
    # ViT –≤—Ö–æ–¥—ã
    vit_input = {
        "input": np.random.randn(1, 3, 32, 32).astype(np.float32)
    }
    
    # –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    try:
        lstm_output = model_manager.predict("lstm", lstm_input)
        print(f"    LSTM –≤—ã—Ö–æ–¥: {list(lstm_output.values())[0].shape}")
        
        vit_output = model_manager.predict("vit", vit_input)
        print(f"    ViT –≤—ã—Ö–æ–¥: {list(vit_output.values())[0].shape}")
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}")
    
    # –ê–Ω—Å–∞–º–±–ª–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
    try:
        model_inputs = {
            "lstm": lstm_input,
            "vit": vit_input
        }
        
        ensemble_output, individual_outputs = ensemble_inference.predict_with_ensemble(model_inputs)
        print(f"    –ê–Ω—Å–∞–º–±–ª—å –≤—ã—Ö–æ–¥: {ensemble_output.shape}")
        print(f"    –ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –≤—ã—Ö–æ–¥—ã: {list(individual_outputs.keys())}")
    except Exception as e:
        print(f"    –û—à–∏–±–∫–∞ –∞–Ω—Å–∞–º–±–ª–µ–≤–æ–≥–æ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {e}")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ latency
    print("  –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ latency...")
    latency_stats = model_manager.get_all_latency_stats()
    for model_name, stats in latency_stats.items():
        if stats:
            print(f"    {model_name}: mean={stats['mean']:.4f}s, p95={stats['p95']:.4f}s")
    
    print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")