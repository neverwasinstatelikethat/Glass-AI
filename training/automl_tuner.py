"""
AutoML tuner –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–π –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–µ–π
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Optuna –¥–ª—è –ø–æ–∏—Å–∫–∞ –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã—Ö –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–π
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import optuna
from optuna.trial import TrialState
import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Any, Callable
import logging
from datetime import datetime
import json
import mlflow
import mlflow.pytorch

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelTrainer:
    """–ë–∞–∑–æ–≤—ã–π —Ç—Ä–µ–Ω–µ—Ä –¥–ª—è –º–æ–¥–µ–ª–µ–π PyTorch"""
    
    def __init__(
        self,
        model: nn.Module,
        train_loader: DataLoader,
        val_loader: DataLoader,
        criterion: nn.Module,
        optimizer: optim.Optimizer,
        device: torch.device,
        experiment_name: str = "glass_defect_prediction"
    ):
        self.model = model
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.criterion = criterion
        self.optimizer = optimizer
        self.device = device
        self.experiment_name = experiment_name
        
        # –ü–µ—Ä–µ–Ω–æ—Å –º–æ–¥–µ–ª–∏ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        self.model.to(self.device)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MLflow
        mlflow.set_experiment(experiment_name)
    
    def train_epoch(self) -> float:
        """–û–±—É—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π —ç–ø–æ—Ö–∏"""
        self.model.train()
        total_loss = 0.0
        total_samples = 0
        
        for batch_idx, (data, target) in enumerate(self.train_loader):
            data, target = data.to(self.device), target.to(self.device)
            
            self.optimizer.zero_grad()
            output = self.model(data)
            loss = self.criterion(output, target)
            loss.backward()
            self.optimizer.step()
            
            total_loss += loss.item() * data.size(0)
            total_samples += data.size(0)
        
        avg_loss = total_loss / total_samples
        return avg_loss
    
    def validate(self) -> Tuple[float, float]:
        """–í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏"""
        self.model.eval()
        total_loss = 0.0
        correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                output = self.model(data)
                loss = self.criterion(output, target)
                
                total_loss += loss.item() * data.size(0)
                pred = output.argmax(dim=1, keepdim=True)
                correct += pred.eq(target.view_as(pred)).sum().item()
                total_samples += data.size(0)
        
        avg_loss = total_loss / total_samples
        accuracy = correct / total_samples
        
        return avg_loss, accuracy
    
    def train(
        self,
        epochs: int = 10,
        patience: int = 5,
        min_delta: float = 1e-4
    ) -> Dict[str, List[float]]:
        """–ü–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å early stopping"""
        train_losses = []
        val_losses = []
        val_accuracies = []
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        # –ù–∞—á–∞–ª–æ MLflow run
        with mlflow.start_run():
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥–µ–ª–∏
            mlflow.log_param("model_type", self.model.__class__.__name__)
            mlflow.log_param("epochs", epochs)
            mlflow.log_param("patience", patience)
            
            for epoch in range(epochs):
                # –û–±—É—á–µ–Ω–∏–µ
                train_loss = self.train_epoch()
                train_losses.append(train_loss)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è
                val_loss, val_accuracy = self.validate()
                val_losses.append(val_loss)
                val_accuracies.append(val_accuracy)
                
                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –º–µ—Ç—Ä–∏–∫
                mlflow.log_metric("train_loss", train_loss, step=epoch)
                mlflow.log_metric("val_loss", val_loss, step=epoch)
                mlflow.log_metric("val_accuracy", val_accuracy, step=epoch)
                
                logger.info(f"Epoch {epoch+1}/{epochs}: "
                           f"Train Loss: {train_loss:.4f}, "
                           f"Val Loss: {val_loss:.4f}, "
                           f"Val Acc: {val_accuracy:.4f}")
                
                # Early stopping
                if val_loss < best_val_loss - min_delta:
                    best_val_loss = val_loss
                    patience_counter = 0
                    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
                    torch.save(self.model.state_dict(), "best_model.pth")
                else:
                    patience_counter += 1
                    if patience_counter >= patience:
                        logger.info(f"Early stopping at epoch {epoch+1}")
                        break
            
            # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏
            mlflow.log_artifact("best_model.pth")
        
        return {
            "train_losses": train_losses,
            "val_losses": val_losses,
            "val_accuracies": val_accuracies
        }


class OptunaLSTMTuner:
    """Tuner –¥–ª—è LSTM –º–æ–¥–µ–ª–∏ —Å Attention"""
    
    def __init__(
        self,
        train_data: Tuple[np.ndarray, np.ndarray],
        val_data: Tuple[np.ndarray, np.ndarray],
        device: torch.device
    ):
        self.train_data = train_data
        self.val_data = val_data
        self.device = device
    
    def objective(self, trial: optuna.Trial) -> float:
        """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        hidden_size = trial.suggest_categorical("hidden_size", [64, 128, 256])
        num_layers = trial.suggest_int("num_layers", 1, 3)
        dropout = trial.suggest_float("dropout", 0.1, 0.5)
        learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
        batch_size = trial.suggest_categorical("batch_size", [16, 32, 64])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        from models.lstm_predictor.attention_lstm import create_lstm_model
        model = create_lstm_model(
            input_size=self.train_data[0].shape[2],
            hidden_size=hidden_size,
            num_layers=num_layers,
            output_size=self.train_data[1].shape[1],
            dropout=dropout
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        train_dataset = TensorDataset(
            torch.FloatTensor(self.train_data[0]), 
            torch.LongTensor(self.train_data[1])
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(self.val_data[0]), 
            torch.LongTensor(self.val_data[1])
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        trainer = ModelTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            optimizer=optimizer,
            device=self.device
        )
        
        # –û–±—É—á–µ–Ω–∏–µ (3 —ç–ø–æ—Ö–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏)
        history = trainer.train(epochs=3, patience=2)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
        return max(history["val_accuracies"])


class OptunaViTTuner:
    """Tuner –¥–ª—è Vision Transformer"""
    
    def __init__(
        self,
        train_data: Tuple[np.ndarray, np.ndarray],
        val_data: Tuple[np.ndarray, np.ndarray],
        device: torch.device
    ):
        self.train_data = train_data
        self.val_data = val_data
        self.device = device
    
    def objective(self, trial: optuna.Trial) -> float:
        """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        embed_dim = trial.suggest_categorical("embed_dim", [256, 512, 768])
        depth = trial.suggest_int("depth", 6, 12)
        n_heads = trial.suggest_categorical("n_heads", [4, 8, 12])
        dropout = trial.suggest_float("dropout", 0.1, 0.5)
        learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
        batch_size = trial.suggest_categorical("batch_size", [8, 16, 32])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        from models.vision_transformer.defect_detector import create_vit_classifier
        model = create_vit_classifier(
            img_size=224,
            patch_size=16,
            in_channels=3,
            n_classes=self.train_data[1].shape[1],
            embed_dim=embed_dim,
            depth=depth,
            n_heads=n_heads
        )
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –∑–∞–≥—Ä—É–∑—á–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
        train_dataset = TensorDataset(
            torch.FloatTensor(self.train_data[0]), 
            torch.LongTensor(self.train_data[1])
        )
        val_dataset = TensorDataset(
            torch.FloatTensor(self.val_data[0]), 
            torch.LongTensor(self.val_data[1])
        )
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ —Ñ—É–Ω–∫—Ü–∏—è –ø–æ—Ç–µ—Ä—å
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.CrossEntropyLoss()
        
        # –¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        trainer = ModelTrainer(
            model=model,
            train_loader=train_loader,
            val_loader=val_loader,
            criterion=criterion,
            optimizer=optimizer,
            device=self.device
        )
        
        # –û–±—É—á–µ–Ω–∏–µ (3 —ç–ø–æ—Ö–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏)
        history = trainer.train(epochs=3, patience=2)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à—É—é –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é —Ç–æ—á–Ω–æ—Å—Ç—å
        return max(history["val_accuracies"])


class OptunaGNNTuner:
    """Tuner –¥–ª—è GNN –º–æ–¥–µ–ª–∏"""
    
    def __init__(
        self,
        train_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
        val_data: List[Tuple[torch.Tensor, torch.Tensor, torch.Tensor]],
        device: torch.device
    ):
        self.train_data = train_data
        self.val_data = val_data
        self.device = device
    
    def objective(self, trial: optuna.Trial) -> float:
        """–¶–µ–ª–µ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"""
        # –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
        hidden_dim = trial.suggest_categorical("hidden_dim", [32, 64, 128])
        num_layers = trial.suggest_int("num_layers", 2, 4)
        dropout = trial.suggest_float("dropout", 0.1, 0.5)
        learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)
        gnn_type = trial.suggest_categorical("gnn_type", ["GCN", "GAT"])
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        from models.gnn_sensor_network.gnn_model import create_sensor_gnn
        model = create_sensor_gnn(
            num_sensors=10,  # –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–∞—Ç—á–∏–∫–æ–≤
            input_dim=1,
            hidden_dim=hidden_dim,
            output_dim=32,
            model_type=gnn_type
        )
        
        # –î–ª—è GNN –Ω—É–∂–Ω–∞ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        # –ó–¥–µ—Å—å —É–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –ø—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –¥–∞–Ω–Ω—ã–µ —É–∂–µ –≤ –Ω—É–∂–Ω–æ–º —Ñ–æ—Ä–º–∞—Ç–µ
        # –í —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞ –∏ —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()  # –î–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –∞–Ω–æ–º–∞–ª–∏–π
        
        # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞
        model.to(self.device)
        model.train()
        
        total_loss = 0.0
        for epoch in range(3):  # 3 —ç–ø–æ—Ö–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏
            for data_tuple in self.train_data:
                x, edge_index, target = data_tuple
                x, edge_index, target = x.to(self.device), edge_index.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(x, edge_index)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–π loss (—Ç–∞–∫ –∫–∞–∫ optuna –º–∞–∫—Å–∏–º–∏–∑–∏—Ä—É–µ—Ç)
        return -total_loss / len(self.train_data)


def run_optuna_study(
    tuner_class: type,
    tuner_params: Dict[str, Any],
    n_trials: int = 20,
    study_name: str = "model_optimization"
) -> optuna.Study:
    """–ó–∞–ø—É—Å–∫ Optuna study –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    # –°–æ–∑–¥–∞–Ω–∏–µ tuner
    tuner = tuner_class(**tuner_params)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ study
    study = optuna.create_study(
        direction="maximize",
        study_name=study_name,
        storage="sqlite:///optuna_study.db",
        load_if_exists=True
    )
    
    # –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
    study.optimize(tuner.objective, n_trials=n_trials)
    
    # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    logger.info(f"–õ—É—á—à–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã {study_name}: {study.best_params}")
    logger.info(f"–õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ: {study.best_value}")
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    with open(f"{study_name}_best_params.json", "w") as f:
        json.dump(study.best_params, f, indent=2)
    
    return study


def create_automl_pipeline(
    train_data: Dict[str, Any],
    val_data: Dict[str, Any],
    device: torch.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
) -> Dict[str, optuna.Study]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –ø–æ–ª–Ω–æ–≥–æ AutoML pipeline –¥–ª—è –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    
    studies = {}
    
    # LSTM tuner
    if "lstm" in train_data:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ AutoML –¥–ª—è LSTM –º–æ–¥–µ–ª–∏...")
        lstm_study = run_optuna_study(
            tuner_class=OptunaLSTMTuner,
            tuner_params={
                "train_data": train_data["lstm"],
                "val_data": val_data["lstm"],
                "device": device
            },
            n_trials=15,
            study_name="lstm_attention_optimization"
        )
        studies["lstm"] = lstm_study
    
    # ViT tuner
    if "vit" in train_data:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ AutoML –¥–ª—è ViT –º–æ–¥–µ–ª–∏...")
        vit_study = run_optuna_study(
            tuner_class=OptunaViTTuner,
            tuner_params={
                "train_data": train_data["vit"],
                "val_data": val_data["vit"],
                "device": device
            },
            n_trials=12,
            study_name="vit_optimization"
        )
        studies["vit"] = vit_study
    
    # GNN tuner
    if "gnn" in train_data:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ AutoML –¥–ª—è GNN –º–æ–¥–µ–ª–∏...")
        gnn_study = run_optuna_study(
            tuner_class=OptunaGNNTuner,
            tuner_params={
                "train_data": train_data["gnn"],
                "val_data": val_data["gnn"],
                "device": device
            },
            n_trials=10,
            study_name="gnn_optimization"
        )
        studies["gnn"] = gnn_study
    
    logger.info("‚úÖ AutoML pipeline –∑–∞–≤–µ—Ä—à–µ–Ω")
    return studies


# ==================== –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï ====================

if __name__ == "__main__":
    # –°–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    print("üîç –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    
    # LSTM –¥–∞–Ω–Ω—ã–µ (–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Ä—è–¥—ã)
    seq_len = 60
    n_features = 20
    n_classes = 5
    n_samples = 1000
    
    lstm_train_X = np.random.randn(n_samples, seq_len, n_features).astype(np.float32)
    lstm_train_y = np.random.randint(0, n_classes, n_samples)
    lstm_val_X = np.random.randn(200, seq_len, n_features).astype(np.float32)
    lstm_val_y = np.random.randint(0, n_classes, 200)
    
    # ViT –¥–∞–Ω–Ω—ã–µ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)
    img_size = 224
    vit_train_X = np.random.randn(800, 3, img_size, img_size).astype(np.float32)
    vit_train_y = np.random.randint(0, n_classes, 800)
    vit_val_X = np.random.randn(200, 3, img_size, img_size).astype(np.float32)
    vit_val_y = np.random.randint(0, n_classes, 200)
    
    # GNN –¥–∞–Ω–Ω—ã–µ (–≥—Ä–∞—Ñ—ã)
    # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è - –≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ –ø–æ—Ç—Ä–µ–±—É—é—Ç—Å—è —Ä–µ–∞–ª—å–Ω—ã–µ –≥—Ä–∞—Ñ—ã
    gnn_train_data = [
        (torch.randn(10, 1), torch.randint(0, 2, (2, 15)), torch.randn(10, 32))
        for _ in range(100)
    ]
    gnn_val_data = [
        (torch.randn(10, 1), torch.randint(0, 2, (2, 15)), torch.randn(10, 32))
        for _ in range(20)
    ]
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_data = {
        "lstm": (lstm_train_X, lstm_train_y),
        "vit": (vit_train_X, vit_train_y),
        "gnn": gnn_train_data
    }
    
    val_data = {
        "lstm": (lstm_val_X, lstm_val_y),
        "vit": (vit_val_X, vit_val_y),
        "gnn": gnn_val_data
    }
    
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AutoML pipeline (—Å —É–º–µ–Ω—å—à–µ–Ω–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º trials)
    print("üîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ AutoML pipeline...")
    
    # –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —É–º–µ–Ω—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ trials
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    # LSTM tuner test
    print("  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LSTM tuner...")
    lstm_tuner = OptunaLSTMTuner(
        train_data=train_data["lstm"],
        val_data=val_data["lstm"],
        device=device
    )
    
    # –ó–∞–ø—É—Å–∫ –æ–¥–Ω–æ–≥–æ trial –¥–ª—è —Ç–µ—Å—Ç–∞
    study = optuna.create_study(direction="maximize")
    study.optimize(lstm_tuner.objective, n_trials=1)
    print(f"    –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (LSTM): {study.best_value}")
    
    # ViT tuner test
    print("  –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ ViT tuner...")
    vit_tuner = OptunaViTTuner(
        train_data=train_data["vit"],
        val_data=val_data["vit"],
        device=device
    )
    
    study = optuna.create_study(direction="maximize")
    study.optimize(vit_tuner.objective, n_trials=1)
    print(f"    –õ—É—á—à–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ (ViT): {study.best_value}")
    
    print("\n‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã!")