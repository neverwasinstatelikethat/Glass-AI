"""
FastAPI Backend –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø—Ä–µ–¥–∏–∫—Ç–∏–≤–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞ —Å—Ç–µ–∫–ª–∞
–ü–æ–¥–¥–µ—Ä–∂–∫–∞ REST API, WebSocket, –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å–æ –≤—Å–µ–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏
"""

import sys
import os
# Add the parent directory to the Python path
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, parent_dir)

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from typing import List, Dict, Optional, Any
from datetime import datetime, timedelta
import asyncio
import logging
import json
import numpy as np

# Import database clients
from storage.influxdb_client import GlassInfluxDBClient
from storage.postgres_client import GlassPostgresClient

# Import synthetic data generator and WebSocket broadcaster
from data_ingestion.synthetic_data_generator import GlassProductionDataGenerator
from streaming_pipeline.websocket_broadcaster import WebSocketBroadcaster, start_background_tasks as start_ws_tasks

# Import Digital Twin Shadow Mode and What-If Analyzer
from simulation.shadow_mode import ShadowModeSimulator
from simulation.what_if_analyzer import WhatIfAnalyzer, ParameterType

# Import Knowledge Graph components
from knowledge_graph.knowledge_base_initializer import initialize_knowledge_base
from knowledge_graph.root_cause_analyzer import analyze_root_cause

# Import Pipeline Orchestrator
from integration.pipeline_orchestrator import create_pipeline_orchestrator

# –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ —Å–∏—Å—Ç–µ–º—ã
from contextlib import asynccontextmanager

# Import unified system
from unified_main import UnifiedGlassProductionSystem

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# ==================== PYDANTIC MODELS ====================

class SensorReading(BaseModel):
    """–ú–æ–¥–µ–ª—å –ø–æ–∫–∞–∑–∞–Ω–∏–π –¥–∞—Ç—á–∏–∫–∞"""
    timestamp: datetime
    production_line: str = "Line_A"
    sensors: Dict[str, Any]
    quality: Optional[Dict[str, Any]] = None


class DefectData(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–∞–Ω–Ω—ã—Ö –æ –¥–µ—Ñ–µ–∫—Ç–µ"""
    timestamp: datetime
    production_line: str
    defect_type: str
    severity: str = Field(..., pattern="^(LOW|MEDIUM|HIGH|CRITICAL)$")
    position: Dict[str, float]
    size_mm: float
    confidence: float = Field(..., ge=0.0, le=1.0)


class PredictionRequest(BaseModel):
    """–ó–∞–ø—Ä–æ—Å –ø—Ä–æ–≥–Ω–æ–∑–∞"""
    horizon_hours: int = Field(default=1, ge=1, le=24)
    production_line: str = "Line_A"
    include_confidence: bool = True


class PredictionResponse(BaseModel):
    """–û—Ç–≤–µ—Ç —Å –ø—Ä–æ–≥–Ω–æ–∑–æ–º"""
    timestamp: datetime
    horizon_hours: int
    predictions: List[Dict[str, Any]]
    confidence: Optional[Dict[str, float]] = None
    recommendations: List[str]


class AlertModel(BaseModel):
    """–ú–æ–¥–µ–ª—å –∞–ª–µ—Ä—Ç–∞"""
    alert_id: str
    timestamp: datetime
    priority: str = Field(..., pattern="^(LOW|MEDIUM|HIGH|CRITICAL)$")
    alert_type: str
    message: str
    affected_sensors: List[str]
    recommended_actions: List[str]


class RecommendationModel(BaseModel):
    """–ú–æ–¥–µ–ª—å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏"""
    recommendation_id: str
    timestamp: datetime
    action_type: str
    description: str
    urgency: str
    expected_impact: str
    confidence: float


class QualityMetrics(BaseModel):
    """–ú–æ–¥–µ–ª—å –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    timestamp: datetime
    production_line: str
    total_units: int
    defect_count: int
    quality_rate: float
    defect_breakdown: Dict[str, int]


class ParameterValues(BaseModel):
    """–ú–æ–¥–µ–ª—å —Ç–µ–∫—É—â–∏—Ö –∑–Ω–∞—á–µ–Ω–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    parameter_values: Dict[str, float]


# ==================== CONNECTION MANAGER ====================

class WebSocketManager:
    """–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ WebSocket –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è–º–∏"""
    
    def __init__(self):
        self.active_connections: List[WebSocket] = []
        self.connection_data: Dict[WebSocket, Dict] = {}    
    async def connect(self, websocket: WebSocket, client_info: Dict = None):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –Ω–æ–≤–æ–≥–æ –∫–ª–∏–µ–Ω—Ç–∞"""
        await websocket.accept()
        self.active_connections.append(websocket)
        self.connection_data[websocket] = client_info or {}
        logger.info(f"‚úÖ WebSocket –∫–ª–∏–µ–Ω—Ç –ø–æ–¥–∫–ª—é—á–µ–Ω. –í—Å–µ–≥–æ: {len(self.active_connections)}")
    
    def disconnect(self, websocket: WebSocket):
        """–û—Ç–∫–ª—é—á–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞"""
        if websocket in self.active_connections:
            self.active_connections.remove(websocket)
            self.connection_data.pop(websocket, None)
            logger.info(f"‚ùå WebSocket –∫–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–µ–Ω. –û—Å—Ç–∞–ª–æ—Å—å: {len(self.active_connections)}")
    
    async def send_personal_message(self, message: Dict, websocket: WebSocket):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–º—É –∫–ª–∏–µ–Ω—Ç—É"""
        try:
            await websocket.send_json(message)
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–æ–±—â–µ–Ω–∏—è: {e}")
            self.disconnect(websocket)
    
    async def broadcast(self, message: Dict, exclude: Optional[WebSocket] = None):
        """Broadcast —Å–æ–æ–±—â–µ–Ω–∏—è –≤—Å–µ–º –∫–ª–∏–µ–Ω—Ç–∞–º"""
        disconnected = []
        
        for connection in self.active_connections:
            if connection == exclude:
                continue
            
            try:
                await connection.send_json(message)
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ broadcast: {e}")
                disconnected.append(connection)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –æ—Ç–∫–ª—é—á–µ–Ω–Ω—ã—Ö
        for conn in disconnected:
            self.disconnect(conn)


# ==================== APPLICATION LIFECYCLE ====================

class ApplicationState:
    """–ì–ª–æ–±–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è"""
    
    def __init__(self):
        self.ws_manager = WebSocketManager()
        # Initialize system integrator with auto-detected config
        self.system_integrator = UnifiedGlassProductionSystem()
        # Initialize synthetic data generator
        self.data_generator = GlassProductionDataGenerator()
        # Initialize WebSocket broadcaster
        self.ws_broadcaster = WebSocketBroadcaster()
        # Initialize notification manager
        self.notification_manager = None  # Will be initialized later
        # Initialize analytics storage for real-time data
        self.analytics_storage = {
            'defects': [],
            'quality_metrics': [],
            'parameter_history': []
        }
        # Initialize Digital Twin components
        self.shadow_mode = None  # Will be initialized after system_integrator
        self.what_if_analyzer = None  # Will be initialized after system_integrator
        self.pipeline_orchestrator = None  # Pipeline orchestrator for end-to-end integration
        self.background_tasks = []
        self.cache = {}
        self.metrics = {
            "total_predictions": 0,
            "total_alerts": 0,
            "uptime_start": datetime.utcnow().isoformat()
        }
        self.kafka_available = False
        # Database clients
        self.influxdb_client = GlassInfluxDBClient()
        # Initialize PostgreSQL client with connection URL from environment
        postgres_url = os.getenv("POSTGRES_URL")
        if postgres_url:
            self.postgres_client = GlassPostgresClient(connection_url=postgres_url)
        else:
            self.postgres_client = GlassPostgresClient()


app_state = ApplicationState()


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Lifecycle manager –¥–ª—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –∏ –æ—á–∏—Å—Ç–∫–∏"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è...")
    
    # Initialize database connections
    try:
        await app_state.influxdb_client.connect()
        await app_state.postgres_client.connect()
        logger.info("‚úÖ Database connections initialized")
    except Exception as e:
        logger.error(f"‚ùå Database connection initialization failed: {e}")
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤
    try:
        # Initialize unified system
        await app_state.system_integrator.initialize_system()
        
        # Check if Kafka is available (safely)
        try:
            # Check if Kafka is available through the data system
            if (hasattr(app_state.system_integrator, 'data_system') and
                app_state.system_integrator.data_system and
                hasattr(app_state.system_integrator.data_system, 'data_router') and
                app_state.system_integrator.data_system.data_router and
                app_state.system_integrator.data_system.data_router.destinations.get("kafka")):
                app_state.kafka_available = True
                logger.info("‚úÖ Kafka –¥–æ—Å—Ç—É–ø–µ–Ω")
            else:
                app_state.kafka_available = False
                logger.warning("‚ö†Ô∏è Kafka –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, —Ä–∞–±–æ—Ç–∞ –≤ —Ä–µ–∂–∏–º–µ —Å–∏–º—É–ª—è—Ü–∏–∏")
        except Exception as e:
            logger.error(f"‚ùå Error checking Kafka availability: {e}")
    
    except Exception as e:
        logger.error(f"‚ùå Error initializing system components: {e}")
    
    # Initialize Pipeline Orchestrator (Phases 5-8)
    try:
        logger.info("üîÑ Initializing Pipeline Orchestrator...")
        app_state.pipeline_orchestrator = create_pipeline_orchestrator(app_state.system_integrator)
        await app_state.pipeline_orchestrator.initialize()
        logger.info("‚úÖ Pipeline Orchestrator initialized (Phases 5-8 active)")
    except Exception as e:
        logger.error(f"‚ùå Error initializing Pipeline Orchestrator: {e}")
    
    # Initialize Notification Manager and integrate with app_state
    try:
        logger.info("üîî Initializing Notification Manager...")
        app_state.notification_manager = notification_manager  # Use global instance
        # NOTE: No sample notifications - all notifications are ML-driven from WebSocket broadcaster
        logger.info("‚úÖ Notification Manager initialized (ML-driven mode - no hardcoded samples)")
    except Exception as e:
        logger.error(f"‚ùå Error initializing Notification Manager: {e}")
    
    # Start WebSocket background tasks with full integration
    try:
        logger.info("üîÑ Starting WebSocket background tasks with notification and analytics integration...")
        ws_tasks = await start_ws_tasks(
            app_state.ws_broadcaster, 
            app_state.data_generator,
            app_state.notification_manager,
            app_state.analytics_storage
        )
        app_state.background_tasks.extend(ws_tasks)
        logger.info(f"‚úÖ Started {len(ws_tasks)} WebSocket background tasks with full integration")
    except Exception as e:
        logger.error(f"‚ùå Error starting WebSocket tasks: {e}")
    
    yield  # Application is running
    
    # Cleanup
    logger.info("üßπ –û—á–∏—Å—Ç–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤...")
    
    # Cancel background tasks
    for task in app_state.background_tasks:
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass
    try:
        await app_state.influxdb_client.close()
        await app_state.postgres_client.close()
        logger.info("‚úÖ Database connections closed")
    except Exception as e:
        logger.error(f"‚ùå Error closing database connections: {e}")


# ==================== FASTAPI APP ====================

app = FastAPI(
    title="Glass Production Predictive Analytics API",
    description="API –¥–ª—è –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–æ–≥–æ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –¥–µ—Ñ–µ–∫—Ç–æ–≤ –≤ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ —Å—Ç–µ–∫–ª–∞",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== REST API ENDPOINTS ====================

@app.get("/")
async def root():
    """–ö–æ—Ä–Ω–µ–≤–æ–π endpoint"""
    return {
        "service": "Glass Production Predictive Analytics",
        "version": "1.0.0",
        "status": "operational",
        "uptime": str(datetime.utcnow() - datetime.fromisoformat(app_state.metrics["uptime_start"]))
    }


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    system_status = await app_state.system_integrator.get_system_status()
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "uptime": str(datetime.utcnow() - datetime.fromisoformat(app_state.metrics["uptime_start"])),
        "components": system_status["components"],
        "kafka_available": app_state.kafka_available
    }


@app.post("/api/sensors/data", response_model=Dict)
async def ingest_sensor_data(data: SensorReading, background_tasks: BackgroundTasks):
    """–ü—Ä–∏–µ–º –¥–∞–Ω–Ω—ã—Ö –æ—Ç –¥–∞—Ç—á–∏–∫–æ–≤"""
    try:
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –≤ —Ñ–æ–Ω–µ
        background_tasks.add_task(process_sensor_data, data.dict())
        
        # Broadcast –≤ WebSocket
        await app_state.ws_manager.broadcast({
            "type": "sensor_update",
            "data": data.dict()
        })
        
        return {
            "status": "accepted",
            "timestamp": datetime.utcnow().isoformat(),
            "message": "–î–∞–Ω–Ω—ã–µ –¥–∞—Ç—á–∏–∫–æ–≤ –ø—Ä–∏–Ω—è—Ç—ã –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É"
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏–µ–º–∞ –¥–∞–Ω–Ω—ã—Ö: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/defects", response_model=Dict)
async def report_defect(defect: DefectData):
    """–†–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏—è –¥–µ—Ñ–µ–∫—Ç–∞"""
    try:
        defect_dict = defect.dict()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö
        # await save_defect_to_db(defect_dict)
        
        # Broadcast –∞–ª–µ—Ä—Ç–∞ –µ—Å–ª–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π
        if defect.severity in ["HIGH", "CRITICAL"]:
            alert = {
                "type": "defect_alert",
                "severity": defect.severity,
                "data": defect_dict
            }
            await app_state.ws_manager.broadcast(alert)
        
        app_state.metrics["total_alerts"] += 1
        
        return {
            "status": "registered",
            "defect_id": f"DEF_{int(datetime.utcnow().timestamp())}",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏ –¥–µ—Ñ–µ–∫—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/predict", response_model=PredictionResponse)
async def generate_prediction(request: PredictionRequest):
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –¥–µ—Ñ–µ–∫—Ç–æ–≤"""
    try:
        logger.info(f"üîÆ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∞ –Ω–∞ {request.horizon_hours} —á–∞—Å–æ–≤")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–∞ –æ—Ç —Å–∏—Å—Ç–µ–º—ã
        model_predictions = await app_state.system_integrator.predict_defects(
            horizon_hours=request.horizon_hours,
            production_line=request.production_line,
            include_confidence=request.include_confidence
        )
        
        # Store prediction results in InfluxDB
        try:
            prediction_data = {
                "timestamp": datetime.utcnow().isoformat(),
                "production_line": request.production_line,
                "horizon_hours": request.horizon_hours,
                "predictions": model_predictions.get("predictions", []),
                "confidence": model_predictions.get("confidence", {})
            }
            # This would be stored in a predictions measurement in InfluxDB
            logger.debug("üíæ Prediction results would be stored in database")
        except Exception as e:
            logger.error(f"‚ùå Error storing prediction in database: {e}")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
        response = PredictionResponse(
            timestamp=datetime.utcnow().isoformat(),
            horizon_hours=request.horizon_hours,
            predictions=model_predictions.get("predictions", []),
            confidence=model_predictions.get("confidence"),
            recommendations=model_predictions.get("recommendations", ["No specific recommendations"])
        )
        
        # Broadcast –ø—Ä–æ–≥–Ω–æ–∑–∞
        await app_state.ws_manager.broadcast({
            "type": "new_prediction",
            "data": response.dict()
        })
        
        return response
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/alerts/active", response_model=List[AlertModel])
async def get_active_alerts():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–∫—Ç–∏–≤–Ω—ã—Ö –∞–ª–µ—Ä—Ç–æ–≤"""
    try:
        # Get alerts from PostgreSQL database
        alerts_data = await app_state.postgres_client.get_active_alerts()
        
        # Convert to AlertModel objects
        alerts = []
        for alert_data in alerts_data:
            alert = AlertModel(
                alert_id=f"ALT_{alert_data.get('id', '001')}",
                timestamp=alert_data.get('timestamp', datetime.utcnow().isoformat()),
                priority=alert_data.get('priority', 'MEDIUM'),
                alert_type=alert_data.get('alert_type', 'unknown'),
                message=alert_data.get('message', 'No message'),
                affected_sensors=[],  # Would need to extract from alert data
                recommended_actions=[]  # Would need to generate based on alert type
            )
            alerts.append(alert)
        
        return alerts
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∞–ª–µ—Ä—Ç–æ–≤: {e}")
        # Generate realistic alerts based on current system state
        try:
            # Get current system state to generate realistic alerts
            # Use a safer approach to get digital twin state
            try:
                dt_state = await app_state.system_integrator.get_digital_twin_state()
                if isinstance(dt_state, dict) and "data" in dt_state and "defects" in dt_state["data"]:
                    defects = dt_state["data"]["defects"]
                    alerts = []
                    
                    # Generate alerts based on defect probabilities
                    for defect_type, probability in defects.items():
                        if probability > 0.3:  # High probability defect
                            alert = AlertModel(
                                alert_id=f"ALT_{int(datetime.utcnow().timestamp() * 1000)}",
                                timestamp=datetime.utcnow().isoformat(),
                                priority="HIGH" if probability > 0.5 else "MEDIUM",
                                alert_type=f"{defect_type}_detected",
                                message=f"–û–±–Ω–∞—Ä—É–∂–µ–Ω —Ä–∏—Å–∫ {defect_type} —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é {probability:.2f}",
                                affected_sensors=["furnace_temp_01", "belt_speed_sensor"],
                                recommended_actions=[
                                    f"–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã, –≤–ª–∏—è—é—â–∏–µ –Ω–∞ {defect_type}",
                                    "–ü—Ä–æ–∫–æ–Ω—Å—É–ª—å—Ç–∏—Ä–æ–≤–∞—Ç—å—Å—è —Å –∏–Ω–∂–µ–Ω–µ—Ä–æ–º"
                                ]
                            )
                            alerts.append(alert)
                    
                    if alerts:
                        return alerts
            except AttributeError:
                # Handle case where get_digital_twin_state method doesn't exist
                logger.warning("Digital twin state method not available, using mock data")
                pass
        except Exception as state_error:
            logger.error(f"‚ùå Error generating alerts from system state: {state_error}")
        
        # Final fallback to mock data if all else fails
        return generate_mock_alerts()


# –î–æ–±–∞–≤—å —ç—Ç–∏ endpoints –≤ fastapi_backend.py

# ==================== ENHANCED DIGITAL TWIN API ENDPOINTS ====================

@app.get("/api/digital-twin/physics")
async def get_physics_state():
    """
    Get detailed physics state including thermal fields, viscosity, stress analysis
    """
    try:
        # Get digital twin state
        dt_state = await app_state.system_integrator.get_digital_twin_state()
        
        # Generate thermal field data (simplified for demo)
        nx, ny = 20, 10
        thermal_field = []
        for i in range(nx):
            row = []
            for j in range(ny):
                center_dist = np.sqrt((i - nx/2)**2 + (j - ny/2)**2)
                temp = 1000 + 700 * np.exp(-center_dist / 5)
                row.append(float(temp))
            thermal_field.append(row)
        
        # Calculate viscosity based on temperature
        furnace_temp = dt_state.get('data', {}).get('furnace', {}).get('temperature', 1520)
        viscosity = calculate_glass_viscosity(furnace_temp)
        
        # Stress analysis
        stress_analysis = {
            'thermal_stress': calculate_thermal_stress(thermal_field),
            'max_stress': np.random.uniform(30, 50),  # MPa
            'stress_concentration_factor': 2.5,
            'safety_factor': 1.8
        }
        
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'thermal_field': thermal_field,
            'viscosity': {
                'average': viscosity,
                'range': [viscosity * 0.8, viscosity * 1.2],
                'unit': 'Pa¬∑s'
            },
            'stress_analysis': stress_analysis,
            'material_properties': {
                'density': 2500.0,  # kg/m¬≥
                'thermal_conductivity': 1.4,  # W/(m¬∑K)
                'thermal_expansion': 9e-6,  # 1/K
                'youngs_modulus': 70e9,  # Pa
                'poisson_ratio': 0.22
            }
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting physics state: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/digital-twin/simulate-batch")
async def simulate_batch_production(request: Dict[str, Any]):
    """
    Simulate batch production with given parameters
    
    Request body:
    {
        "furnace_temperature": 1520,
        "belt_speed": 150,
        "mold_temp": 320,
        "batch_size": 100,
        "duration_hours": 8
    }
    """
    try:
        furnace_temp = request.get('furnace_temperature', 1520)
        belt_speed = request.get('belt_speed', 150)
        mold_temp = request.get('mold_temp', 320)
        batch_size = request.get('batch_size', 100)
        duration = request.get('duration_hours', 8)
        
        # Simulate production over time
        timeline = []
        current_quality = 0.95
        total_defects = 0
        
        for hour in range(duration):
            # Quality degradation over time
            quality_factor = 1.0 - (hour * 0.01)
            
            # Calculate defects for this hour
            temp_deviation = abs(furnace_temp - 1520) / 100
            speed_factor = abs(belt_speed - 150) / 50
            
            defect_rate = (temp_deviation + speed_factor) * 0.1 * quality_factor
            defects_this_hour = int(batch_size * defect_rate)
            total_defects += defects_this_hour
            
            current_quality = max(0.5, 1.0 - (total_defects / (batch_size * duration)))
            
            timeline.append({
                'hour': hour,
                'quality_rate': current_quality,
                'defects_cumulative': total_defects,
                'units_produced': batch_size * (hour + 1),
                'furnace_temp': furnace_temp + np.random.randn() * 5,
                'belt_speed': belt_speed + np.random.randn() * 3
            })
        
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'simulation_params': {
                'furnace_temperature': furnace_temp,
                'belt_speed': belt_speed,
                'mold_temp': mold_temp,
                'batch_size': batch_size,
                'duration_hours': duration
            },
            'results': {
                'total_units': batch_size * duration,
                'total_defects': total_defects,
                'final_quality_rate': current_quality,
                'defect_rate': total_defects / (batch_size * duration)
            },
            'timeline': timeline
        }
    except Exception as e:
        logger.error(f"‚ùå Error simulating batch production: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/digital-twin/defect-visualization")
async def get_defect_visualization_data():
    """
    Get 3D spatial defect data for visualization
    """
    try:
        dt_state = await app_state.system_integrator.get_digital_twin_state()
        defects = dt_state.get('data', {}).get('defects', {})
        
        # Generate spatial defect positions
        defect_particles = []
        
        for defect_type, probability in defects.items():
            # Number of particles based on probability
            num_particles = int(probability * 50)
            
            for _ in range(num_particles):
                # Random 3D position
                x = np.random.uniform(-10, 10)
                y = np.random.uniform(0, 5)
                z = np.random.uniform(-5, 5)
                
                # Color based on defect type
                color_map = {
                    'crack': '#EF5350',
                    'bubble': '#FFA726',
                    'chip': '#FF6B35',
                    'cloudiness': '#AAAAAA',
                    'deformation': '#9C27B0',
                    'stress': '#E91E63'
                }
                
                defect_particles.append({
                    'type': defect_type,
                    'position': [float(x), float(y), float(z)],
                    'size': float(np.random.uniform(0.05, 0.15)),
                    'color': color_map.get(defect_type, '#FFFFFF'),
                    'severity': float(probability)
                })
        
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'defect_particles': defect_particles,
            'defect_statistics': defects,
            'total_particles': len(defect_particles)
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting defect visualization: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/digital-twin/thermal-map")
async def get_thermal_map(resolution: int = 20):
    """
    Get 2D thermal map of the furnace
    
    Query Parameters:
        resolution: Grid resolution (default 20x20)
    """
    try:
        dt_state = await app_state.system_integrator.get_digital_twin_state()
        furnace_temp = dt_state.get('data', {}).get('furnace', {}).get('temperature', 1520)
        
        # Generate thermal map
        thermal_map = []
        for i in range(resolution):
            row = []
            for j in range(resolution):
                # Center is hottest
                center_dist = np.sqrt(
                    (i - resolution/2)**2 + (j - resolution/2)**2
                )
                
                # Temperature distribution
                temp = 1000 + (furnace_temp - 1000) * np.exp(-center_dist / (resolution / 3))
                
                # Add some noise
                temp += np.random.randn() * 10
                
                row.append(float(temp))
            thermal_map.append(row)
        
        return {
            'timestamp': datetime.utcnow().isoformat(),
            'resolution': resolution,
            'thermal_map': thermal_map,
            'average_temp': float(np.mean(thermal_map)),
            'max_temp': float(np.max(thermal_map)),
            'min_temp': float(np.min(thermal_map)),
            'temperature_gradient': float(np.max(thermal_map) - np.min(thermal_map))
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting thermal map: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# Helper functions
def calculate_glass_viscosity(temperature: float) -> float:
    """Calculate glass viscosity using Arrhenius equation"""
    A = 1e-2  # Pre-exponential factor
    Ea = 300000.0  # Activation energy (J/mol)
    R = 8.314  # Gas constant
    T_K = temperature + 273.15  # Convert to Kelvin
    
    viscosity = A * np.exp(Ea / (R * T_K))
    return float(np.clip(viscosity, 100, 10000))


def calculate_thermal_stress(thermal_field: List[List[float]]) -> float:
    """Calculate thermal stress from temperature field"""
    field_array = np.array(thermal_field)
    
    # Calculate temperature gradients
    grad_x = np.gradient(field_array, axis=0)
    grad_y = np.gradient(field_array, axis=1)
    
    # Maximum gradient magnitude
    gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)
    max_gradient = float(np.max(gradient_magnitude))
    
    # Thermal stress proportional to gradient
    E = 70e9  # Young's modulus
    alpha = 9e-6  # Thermal expansion coefficient
    
    thermal_stress = E * alpha * max_gradient / 1e6  # Convert to MPa
    
    return float(thermal_stress)


@app.get("/api/recommendations", response_model=List[RecommendationModel])
async def get_recommendations():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –¥–ª—è –æ–ø–µ—Ä–∞—Ç–æ—Ä–æ–≤"""
    try:
        # Get recommendations from PostgreSQL database
        rec_data = await app_state.postgres_client.get_pending_recommendations()
        
        # Convert to RecommendationModel objects
        recommendations = []
        for rec in rec_data:
            recommendation = RecommendationModel(
                recommendation_id=f"REC_{rec.get('id', '001')}",
                timestamp=rec.get('timestamp', datetime.utcnow().isoformat()),
                action_type=rec.get('action_type', 'unknown'),
                description=rec.get('description', 'No description'),
                urgency=rec.get('urgency', 'MEDIUM'),
                expected_impact=rec.get('expected_impact', 'Unknown impact'),
                confidence=float(rec.get('confidence', 0.5))
            )
            recommendations.append(recommendation)
        
        return recommendations
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        # Generate realistic recommendations based on current system state
        try:
            # Get RL agent recommendations
            rl_recommendations = await app_state.system_integrator.get_rl_recommendations()
            if "recommendations" in rl_recommendations:
                recommendations = []
                base_timestamp = datetime.utcnow()
                
                for i, rec_item in enumerate(rl_recommendations["recommendations"]):
                    # Handle both string and dictionary recommendations
                    if isinstance(rec_item, dict):
                        # Dictionary format - extract text from action field
                        rec_text = rec_item.get("action", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è")
                    else:
                        # String format
                        rec_text = rec_item
                    
                    recommendation = RecommendationModel(
                        recommendation_id=f"REC_RL_{int(base_timestamp.timestamp() * 1000) + i}",
                        timestamp=base_timestamp.isoformat(),
                        action_type="process_optimization",
                        description=rec_text,
                        urgency="HIGH" if "—É–≤–µ–ª–∏—á–∏—Ç—å" in rec_text.lower() or "—É–º–µ–Ω—å—à–∏—Ç—å" in rec_text.lower() else "MEDIUM",
                        expected_impact="–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—Å–∞",
                        confidence=float(rl_recommendations.get("confidence", 0.8))
                    )
                    recommendations.append(recommendation)
                
                if recommendations:
                    return recommendations
        except Exception as rl_error:
            logger.error(f"‚ùå Error generating recommendations from RL agent: {rl_error}")
        
        # Final fallback to mock data if all else fails
        return generate_mock_recommendations()


@app.get("/api/quality/metrics", response_model=QualityMetrics)
async def get_quality_metrics(production_line: str = "Line_A"):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞"""
    try:
        # Get quality metrics from PostgreSQL database
        metrics_data = await app_state.postgres_client.get_recent_quality_metrics(production_line, limit=1)
        
        if metrics_data:
            # Use latest metrics from database
            latest_metrics = metrics_data[0]
            quality_metrics = QualityMetrics(
                timestamp=latest_metrics.get('timestamp', datetime.utcnow().isoformat()),
                production_line=production_line,
                total_units=latest_metrics.get('total_units', 0),
                defect_count=latest_metrics.get('defective_units', 0),
                quality_rate=float(latest_metrics.get('quality_rate', 0.0)),
                defect_breakdown={}  # Would need to populate from database
            )
        else:
            # Generate realistic quality metrics based on current system state
            try:
                # Get current digital twin state to generate realistic metrics
                dt_state = await app_state.system_integrator.get_digital_twin_state()
                if "data" in dt_state and "quality_score" in dt_state["data"]:
                    quality_score = dt_state["data"]["quality_score"]
                    # Use dynamic units produced based on quality score rather than hardcoded 1000
                    total_units = max(100, int(quality_score * 1200))  # Scale with quality
                    defect_count = int(total_units * (1 - quality_score))
                                    
                    # Generate realistic defect breakdown based on digital twin defects
                    defect_breakdown = {}
                    if "defects" in dt_state["data"]:
                        defects = dt_state["data"]["defects"]
                        for defect_type, probability in defects.items():
                            defect_breakdown[defect_type] = int(probability * 100)  # Scale for display
                                    
                    quality_metrics = QualityMetrics(
                        timestamp=datetime.utcnow().isoformat(),
                        production_line=production_line,
                        total_units=total_units,
                        defect_count=defect_count,
                        quality_rate=float(quality_score * 100),
                        defect_breakdown=defect_breakdown
                    )
                    return quality_metrics
            except Exception as state_error:
                logger.error(f"‚ùå Error generating quality metrics from system state: {state_error}")
                            
            # Fallback to mock data if no system state available
            quality_metrics = QualityMetrics(
                timestamp=datetime.utcnow().isoformat(),
                production_line=production_line,
                total_units=0,  # Changed from hardcoded 1000 to 0
                defect_count=0,  # Changed from hardcoded 25 to 0
                quality_rate=0.0,  # Changed from hardcoded 97.5 to 0.0
                defect_breakdown={}
            )
        
        return quality_metrics
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/statistics")
async def get_statistics():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ —Å–∏—Å—Ç–µ–º—ã"""
    system_status = await app_state.system_integrator.get_system_status()
    return {
        "total_predictions": app_state.metrics["total_predictions"],
        "total_alerts": app_state.metrics["total_alerts"],
        "uptime": str(datetime.utcnow() - datetime.fromisoformat(app_state.metrics["uptime_start"])),
        "active_websocket_connections": len(app_state.ws_manager.active_connections),
        "system_status": system_status,
        "timestamp": datetime.utcnow().isoformat()
    }


@app.get("/api/digital-twin/state")
async def get_digital_twin_state():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –¥–≤–æ–π–Ω–∏–∫–∞"""
    try:
        dt_state = await app_state.system_integrator.get_digital_twin_state()
        return dt_state
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –¥–≤–æ–π–Ω–∏–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/digital-twin/state")
async def get_digital_twin_state_direct():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –¥–≤–æ–π–Ω–∏–∫–∞ (direct endpoint)"""
    try:
        dt_state = await app_state.system_integrator.get_digital_twin_state()
        return dt_state
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ü–∏—Ñ—Ä–æ–≤–æ–≥–æ –¥–≤–æ–π–Ω–∏–∫–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/rl/recommendations")
async def get_rl_recommendations():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –æ—Ç RL –∞–≥–µ–Ω—Ç–∞"""
    try:
        recommendations = await app_state.system_integrator.get_rl_recommendations()
        return recommendations
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π RL: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/predictions")
async def get_model_predictions():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –æ—Ç –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
    try:
        predictions = await app_state.system_integrator.get_model_predictions()
        return predictions
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –º–æ–¥–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/explanations")
async def get_model_explanations():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –¥–ª—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –º–æ–¥–µ–ª–µ–π"""
    try:
        explanations = await app_state.system_integrator.get_model_explanations()
        return explanations
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –æ–±—ä—è—Å–Ω–µ–Ω–∏–π –º–æ–¥–µ–ª–µ–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/performance")
async def get_model_performance():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
    try:
        performance = await app_state.system_integrator.get_model_performance()
        return performance
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/models/info")
async def get_model_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö"""
    try:
        if hasattr(app_state.system_integrator, 'model_manager') and app_state.system_integrator.model_manager:
            info = app_state.system_integrator.model_manager.get_model_info()
            return info
        else:
            raise HTTPException(status_code=503, detail="Model manager not available")
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª—è—Ö: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/causes/{defect}")
async def get_knowledge_graph_causes(defect: str, min_confidence: float = 0.5):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–∏—á–∏–Ω –¥–µ—Ñ–µ–∫—Ç–∞ –∏–∑ Knowledge Graph"""
    try:
        causes = await app_state.system_integrator.get_causes_of_defect(defect, min_confidence)
        if "error" in causes:
            raise HTTPException(status_code=503, detail=causes["error"])
        return causes
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø—Ä–∏—á–∏–Ω –¥–µ—Ñ–µ–∫—Ç–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/knowledge-graph/recommendations/{defect}")
async def get_knowledge_graph_recommendations(defect: str, parameter_values: ParameterValues):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —É—Å—Ç—Ä–∞–Ω–µ–Ω–∏—é –¥–µ—Ñ–µ–∫—Ç–∞"""
    try:
        recommendations = await app_state.system_integrator.get_intervention_recommendations(
            defect, parameter_values.parameter_values
        )
        if "error" in recommendations:
            raise HTTPException(status_code=503, detail=recommendations["error"])
        return recommendations
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/subgraph/{defect}")
async def get_knowledge_graph_subgraph(defect: str, max_depth: int = 2, 
                              include_recommendations: bool = True,
                              include_human_decisions: bool = True):
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø–æ–¥–≥—Ä–∞—Ñ–∞ –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–ª–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
    try:
        subgraph = await app_state.system_integrator.get_knowledge_graph_subgraph(
            defect, max_depth, include_recommendations, include_human_decisions
        )
        if "error" in subgraph:
            raise HTTPException(status_code=503, detail=subgraph["error"])
        return subgraph
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–¥–≥—Ä–∞—Ñ–∞: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== KNOWLEDGE GRAPH ENRICHMENT ENDPOINTS ====================

@app.get("/api/knowledge-graph/defect-recommendations/{defect}")
async def get_defect_recommendation_graph(defect: str, include_decisions: bool = True, max_recs: int = 10):
    """
    Get defect-recommendation relationship graph for visualization.
    Shows defect causes, recommendations, and human decision outcomes.
    """
    try:
        if not hasattr(app_state.system_integrator, 'knowledge_graph') or not app_state.system_integrator.knowledge_graph:
            # Return mock data if KG not available
            return {
                "nodes": [
                    {"id": "defect_1", "label": defect.upper(), "type": "defect", "severity": "HIGH"},
                    {"id": "param_1", "label": "furnace_temperature", "type": "parameter", "confidence": 0.85},
                    {"id": "rec_1", "label": "–°–Ω–∏–∑–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É", "type": "recommendation", "applied": False, "confidence": 0.88}
                ],
                "edges": [
                    {"source": "param_1", "target": "defect_1", "type": "CAUSES", "confidence": 0.85},
                    {"source": "rec_1", "target": "defect_1", "type": "ADDRESSES", "confidence": 0.88}
                ],
                "defect_type": defect,
                "is_mock": True,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        kg = app_state.system_integrator.knowledge_graph
        result = kg.get_defect_recommendation_graph(
            defect_type=defect,
            include_human_decisions=include_decisions,
            max_recommendations=max_recs
        )
        return result
        
    except Exception as e:
        logger.error(f"‚ùå Error getting defect-recommendation graph: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/knowledge-graph/enrich/human-decision")
async def enrich_from_human_decision(request: Dict[str, Any]):
    """
    Enrich Knowledge Graph from human operator decision.
    Used when operator applies, dismisses, or modifies a recommendation.
    
    Request body:
    {
        "notification_id": "notif_123",
        "decision": "applied" | "dismissed" | "modified",
        "defect_type": "crack",
        "recommendation_id": "rl_rec_123" (optional),
        "notes": "operator notes" (optional)
    }
    """
    try:
        notification_id = request.get("notification_id")
        decision = request.get("decision")  # applied, dismissed, modified
        defect_type = request.get("defect_type")
        recommendation_id = request.get("recommendation_id")
        notes = request.get("notes")
        
        if not all([notification_id, decision, defect_type]):
            raise HTTPException(
                status_code=400, 
                detail="Missing required fields: notification_id, decision, defect_type"
            )
        
        if decision not in ["applied", "dismissed", "modified"]:
            raise HTTPException(
                status_code=400,
                detail="decision must be one of: applied, dismissed, modified"
            )
        
        # Enrich KG if available
        result = {"enriched": False, "message": "KG not available"}
        
        if hasattr(app_state.system_integrator, 'knowledge_graph') and app_state.system_integrator.knowledge_graph:
            kg = app_state.system_integrator.knowledge_graph
            result = kg.enrich_from_human_decision(
                notification_id=notification_id,
                decision=decision,
                defect_type=defect_type,
                recommendation_id=recommendation_id,
                notes=notes,
                timestamp=datetime.utcnow()
            )
        
        logger.info(f"üë§ Human decision recorded: {decision} for {defect_type}")
        
        return {
            "status": "success",
            "decision": decision,
            "defect_type": defect_type,
            "kg_enriched": result.get("enriched", False),
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error enriching KG from human decision: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/rl-feedback")
async def get_rl_feedback_history(defect_type: Optional[str] = None, limit: int = 100):
    """
    Get RL feedback history from human decisions for continuous learning.
    Returns feedback records that can be used for RL training.
    """
    try:
        if not hasattr(app_state.system_integrator, 'knowledge_graph') or not app_state.system_integrator.knowledge_graph:
            return {
                "feedback_records": [],
                "count": 0,
                "message": "KG not available"
            }
        
        kg = app_state.system_integrator.knowledge_graph
        feedback = kg.get_rl_feedback_history(defect_type=defect_type, limit=limit)
        
        return {
            "feedback_records": feedback,
            "count": len(feedback),
            "defect_type_filter": defect_type,
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting RL feedback history: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/statistics")
async def get_knowledge_graph_statistics(timeframe: str = "24h"):
    """
    Get Knowledge Graph statistics including node counts, relationship counts, and recent additions.
    Provides real-time metrics with configurable timeframes.
    
    Args:
        timeframe: Time period for analysis (24h, 7d, 30d)
    """
    try:
        if not hasattr(app_state.system_integrator, 'knowledge_graph') or not app_state.system_integrator.knowledge_graph:
            return {
                "nodes": 0,
                "relationships": 0,
                "defects": 0,
                "causes": 0,
                "recommendations": 0,
                "parameters": 0,
                "equipment": 0,
                "human_decisions": 0,
                "recent_causal_links_24h": 0,
                "recent_recommendations_24h": 0,
                "recent_human_decisions_24h": 0,
                "avg_cause_confidence": 0.0,
                "avg_recommendation_confidence": 0.0,
                "enrichment_count_24h": 0,
                "message": "KG not available"
            }
        
        kg = app_state.system_integrator.knowledge_graph
        
        # Determine time filter based on timeframe
        time_filters = {
            "24h": "{hours: 24}",
            "7d": "{days: 7}",
            "30d": "{days: 30}"
        }
        time_filter = time_filters.get(timeframe, "{hours: 24}")
        
        # Get statistics from Neo4j
        with kg.driver.session() as session:
            # Count total nodes by type
            nodes_result = session.run("""
                MATCH (n)
                RETURN 
                    count(n) as total_nodes,
                    count(n:Defect) as defect_nodes,
                    count(n:Cause) as cause_nodes,
                    count(n:Recommendation) as recommendation_nodes,
                    count(n:Parameter) as parameter_nodes,
                    count(n:Equipment) as equipment_nodes,
                    count(n:HumanDecision) as human_decision_nodes
            """)
            
            nodes_record = nodes_result.single()
            
            # Count relationships by type
            rel_result = session.run("""
                MATCH ()-[r]->()
                WITH count(r) as total_relationships
                OPTIONAL MATCH ()-[r1:CAUSES]->()
                WITH total_relationships, count(r1) as causes_relationships
                OPTIONAL MATCH ()-[r2:RELATED_TO]->()
                WITH total_relationships, causes_relationships, count(r2) as related_to_relationships
                OPTIONAL MATCH ()-[r3:FROM_EQUIPMENT]->()
                WITH total_relationships, causes_relationships, related_to_relationships, count(r3) as from_equipment_relationships
                OPTIONAL MATCH ()-[r4:ADDRESSES]->()
                WITH total_relationships, causes_relationships, related_to_relationships, from_equipment_relationships, count(r4) as addresses_relationships
                OPTIONAL MATCH ()-[r5:REGARDING]->()
                RETURN 
                    total_relationships,
                    causes_relationships,
                    related_to_relationships,
                    from_equipment_relationships,
                    addresses_relationships,
                    count(r5) as regarding_relationships
            """)
            
            rel_record = rel_result.single() if rel_result.peek() else {
                "total_relationships": 0,
                "causes_relationships": 0,
                "related_to_relationships": 0,
                "from_equipment_relationships": 0,
                "addresses_relationships": 0,
                "regarding_relationships": 0
            }
            
            # Count recent causal links
            recent_causal_result = session.run(f"""
                MATCH (c:Cause)-[:CAUSES]->(d:Defect)
                WHERE c.timestamp > datetime().minus(duration({time_filter}))
                RETURN count(c) as recent_causal_links
            """)
            recent_causal_record = recent_causal_result.single()
            
            # Count recent recommendations
            recent_rec_result = session.run(f"""
                MATCH (r:Recommendation)
                WHERE r.timestamp > datetime().minus(duration({time_filter}))
                RETURN count(r) as recent_recommendations
            """)
            recent_rec_record = recent_rec_result.single()
            
            # Count recent human decisions
            recent_dec_result = session.run(f"""
                MATCH (h:HumanDecision)
                WHERE h.timestamp > datetime().minus(duration({time_filter}))
                RETURN count(h) as recent_human_decisions
            """)
            recent_dec_record = recent_dec_result.single()
            
            # Trend analysis for causal link discovery
            # Get causal link creation trends over time
            trend_result = session.run(f"""
                MATCH (c:Cause)-[:CAUSES]->(d:Defect)
                WHERE c.timestamp > datetime().minus(duration.days(30))
                WITH date(c.timestamp) as day, count(c) as daily_causal_links
                RETURN day, daily_causal_links
                ORDER BY day
            """)
            
            trend_data = []
            for record in trend_result:
                trend_data.append({
                    "date": record["day"],
                    "causal_links": record["daily_causal_links"]
                })
            
            # Calculate trend direction (increasing/decreasing)
            trend_direction = "stable"
            if len(trend_data) >= 2:
                recent_avg = sum(d["causal_links"] for d in trend_data[-7:]) / min(7, len(trend_data))
                older_avg = sum(d["causal_links"] for d in trend_data[-14:-7]) / min(7, max(1, len(trend_data) - 7))
                if recent_avg > older_avg * 1.1:  # 10% increase threshold
                    trend_direction = "increasing"
                elif recent_avg < older_avg * 0.9:  # 10% decrease threshold
                    trend_direction = "decreasing"
            
            # Calculate average confidence scores
            avg_confidence_result = session.run("""
                MATCH (c:Cause)
                RETURN avg(c.confidence) as avg_cause_confidence
            """)
            avg_confidence_record = avg_confidence_result.single()
            
            avg_rec_confidence_result = session.run("""
                MATCH (r:Recommendation)
                RETURN avg(r.confidence) as avg_recommendation_confidence
            """)
            avg_rec_confidence_record = avg_rec_confidence_result.single()
            
            # Get enrichment metrics from Redis if available
            enrichment_count = 0
            if kg.cache_enabled:
                try:
                    # Get recent ML enrichments
                    today = datetime.utcnow().date().isoformat()
                    enrichment_keys = kg.redis_client.keys(f"ml_enrichment_metrics:*:{today}")
                    enrichment_count = len(enrichment_keys)
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Error getting enrichment metrics: {e}")
            
            return {
                "nodes": {
                    "total": nodes_record["total_nodes"],
                    "defects": nodes_record["defect_nodes"],
                    "causes": nodes_record["cause_nodes"],
                    "recommendations": nodes_record["recommendation_nodes"],
                    "parameters": nodes_record["parameter_nodes"],
                    "equipment": nodes_record["equipment_nodes"],
                    "human_decisions": nodes_record["human_decision_nodes"]
                },
                "relationships": {
                    "total": rel_record["total_relationships"],
                    "causes": rel_record.get("causes_relationships", 0),
                    "related_to": rel_record.get("related_to_relationships", 0),
                    "from_equipment": rel_record.get("from_equipment_relationships", 0),
                    "addresses": rel_record.get("addresses_relationships", 0),
                    "regarding": rel_record.get("regarding_relationships", 0)
                },
                "recent_activity": {
                    "causal_links": recent_causal_record["recent_causal_links"],
                    "recommendations": recent_rec_record["recent_recommendations"],
                    "human_decisions": recent_dec_record["recent_human_decisions"],
                    "enrichments": enrichment_count,
                    "timeframe": timeframe
                },
                "quality_metrics": {
                    "avg_cause_confidence": float(avg_confidence_record["avg_cause_confidence"] or 0.0),
                    "avg_recommendation_confidence": float(avg_rec_confidence_record["avg_recommendation_confidence"] or 0.0)
                },
                "trend_analysis": {
                    "causal_link_discovery": {
                        "trend_data": trend_data,
                        "trend_direction": trend_direction,
                        "last_30_days_total": sum(d["causal_links"] for d in trend_data),
                        "peak_daily_discoveries": max((d["causal_links"] for d in trend_data), default=0)
                    }
                },
                "timestamp": datetime.utcnow().isoformat(),
                "timeframe": timeframe
            }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting Knowledge Graph statistics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== WEBSOCKET ENDPOINT ====================

@app.websocket("/ws/realtime")
async def websocket_realtime_endpoint(websocket: WebSocket):
    """WebSocket endpoint –¥–ª—è —Ä–µ–∞–ª—Ç–∞–π–º –¥–∞–Ω–Ω—ã—Ö"""
    await app_state.ws_manager.connect(websocket, {"connected_at": datetime.utcnow().isoformat()})
    
    try:
        # Send welcome message
        await websocket.send_json({
            "type": "welcome",
            "message": "Connected to real-time monitoring system",
            "timestamp": datetime.utcnow().isoformat()
        })
        
        while True:
            try:
                # Wait for message with timeout
                data = await asyncio.wait_for(websocket.receive_text(), timeout=30.0)
                logger.debug(f"üì• Received WebSocket message: {data}")
                
                # Echo response
                await websocket.send_text(f"Echo: {data}")
                
            except asyncio.TimeoutError:
                # Send heartbeat if no messages for 30 seconds
                await websocket.send_json({
                    "type": "heartbeat",
                    "timestamp": datetime.utcnow().isoformat(),
                    "message": "Connection alive"
                })
            
    except WebSocketDisconnect:
        app_state.ws_manager.disconnect(websocket)
        logger.info("‚ùå WebSocket client disconnected")
    except Exception as e:
        logger.error(f"‚ùå WebSocket error: {e}")
        app_state.ws_manager.disconnect(websocket)


# ==================== BACKGROUND TASKS ====================

async def process_sensor_data(data: Dict):
    """–§–æ–Ω–æ–≤–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–∞—Ç—á–∏–∫–æ–≤"""
    try:
        # Store latest sensor data in system integrator for digital twin
        if hasattr(app_state.system_integrator, '_latest_sensor_data'):
            app_state.system_integrator._latest_sensor_data = data
        else:
            app_state.system_integrator._latest_sensor_data = data
        
        # Feature extraction would happen here
        logger.debug(f"‚úÖ –î–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω—ã: {data['timestamp']}")
        
    except Exception as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏: {e}")


async def realtime_update_broadcaster():
    """–§–æ–Ω–æ–≤–∞—è –∑–∞–¥–∞—á–∞ –¥–ª—è broadcast –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π"""
    while True:
        try:
            # Get real data from system integrator
            # Handle case where digital twin state method might not be available
            dt_state = {}
            try:
                dt_state = await app_state.system_integrator.get_digital_twin_state()
            except AttributeError:
                # If method doesn't exist, use mock data
                logger.warning("Digital twin state method not available, using mock data")
                dt_state = {
                    "data": {
                        "quality_score": 0.965,
                        "defects": {
                            "crack": 0.1,
                            "bubble": 0.15,
                            "chip": 0.05
                        }
                    }
                }
            
            # Extract relevant metrics
            quality_rate = 96.5  # Default value
            defect_count = 5     # Default value
            
            if "data" in dt_state:
                if "quality_score" in dt_state["data"]:
                    quality_rate = float(dt_state["data"]["quality_score"] * 100)
                
                if "defects" in dt_state["data"]:
                    defects = dt_state["data"]["defects"]
                    defect_count = sum(1 for prob in defects.values() if prob > 0.3)
            
            update = {
                "type": "realtime_update",
                "timestamp": datetime.utcnow().isoformat(),
                "data": {
                    "current_quality_rate": quality_rate,
                    "active_alerts": defect_count,
                    "defect_count_hourly": defect_count * 12  # Scale for hourly projection
                }
            }
            
            await app_state.ws_manager.broadcast(update)
            await asyncio.sleep(30)  # Update every 30 seconds instead of 5
            
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ broadcaster: {e}")
            await asyncio.sleep(30)


# ==================== HELPER FUNCTIONS ====================

# ==================== DIGITAL TWIN API ENDPOINTS ====================

@app.get("/api/digital-twin/shadow-state")
async def get_shadow_mode_state():
    """
    Get current Shadow Mode validation metrics
    Shows how well Digital Twin predictions match reality
    """
    if not app_state.shadow_mode:
        return {
            "available": False,
            "message": "Shadow Mode not initialized"
        }
    
    try:
        metrics = app_state.shadow_mode.get_validation_metrics()
        comparison = app_state.shadow_mode.get_state_comparison()
        
        return {
            "status": "operational",
            "timestamp": datetime.utcnow().isoformat(),
            "validation_metrics": metrics,
            "state_comparison": comparison
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting shadow mode state: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/digital-twin/what-if")
async def analyze_what_if_scenario(request: Dict[str, Any]):
    """
    Analyze what-if scenario for parameter changes
    
    Request body:
    {
        "parameter_changes": {
            "furnace_temperature": 1550.0,
            "belt_speed": 140.0
        },
        "scenario_id": "optional_scenario_name"
    }
    """
    if not app_state.what_if_analyzer:
        return {
            "available": False,
            "message": "What-If Analyzer not initialized"
        }
    
    try:
        parameter_changes_raw = request.get("parameter_changes", {})
        scenario_id = request.get("scenario_id", f"scenario_{int(datetime.utcnow().timestamp())}")
        
        # Convert parameter names to ParameterType enum
        parameter_changes = {}
        for param_name, value in parameter_changes_raw.items():
            try:
                param_type = ParameterType(param_name)
                parameter_changes[param_type] = float(value)
            except ValueError:
                logger.warning(f"‚ö†Ô∏è Unknown parameter type: {param_name}")
        
        if not parameter_changes:
            raise HTTPException(status_code=400, detail="No valid parameter changes provided")
        
        # Create and analyze scenario
        scenario = app_state.what_if_analyzer.create_scenario(scenario_id, parameter_changes)
        
        # Convert scenario to response format
        response = {
            "scenario_id": scenario.scenario_id,
            "timestamp": scenario.timestamp.isoformat(),
            "parameter_changes": {p.value: v for p, v in scenario.parameter_changes.items()},
            "baseline_state": {
                "furnace_temperature": scenario.baseline_state.get('furnace', {}).get('temperature'),
                "quality_score": scenario.baseline_state.get('quality_score')
            }
        }
        
        if scenario.predicted_outcome:
            impact = scenario.predicted_outcome.get('impact_analysis')
            if impact:
                response["impact_analysis"] = {
                    "defect_rate_change_percent": impact.defect_rate_change,
                    "quality_score_impact": impact.quality_score_impact,
                    "production_rate_impact_percent": impact.production_rate_impact,
                    "energy_consumption_change_percent": impact.energy_consumption_change,
                    "time_to_effect_minutes": impact.time_to_effect_minutes,
                    "risk_level": impact.risk_level,
                    "warnings": impact.warnings,
                    "recommendations": impact.recommendations
                }
        
        return response
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error analyzing what-if scenario: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/digital-twin/scenarios")
async def list_what_if_scenarios():
    """
    List all what-if scenarios that have been created
    """
    if not app_state.what_if_analyzer:
        return {
            "available": False,
            "scenarios": []
        }
    
    try:
        scenarios = []
        for scenario in app_state.what_if_analyzer.scenario_history:
            scenarios.append({
                "scenario_id": scenario.scenario_id,
                "timestamp": scenario.timestamp.isoformat(),
                "parameter_changes": {p.value: v for p, v in scenario.parameter_changes.items()}
            })
        
        return {
            "total_scenarios": len(scenarios),
            "scenarios": scenarios
        }
    except Exception as e:
        logger.error(f"‚ùå Error listing scenarios: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/digital-twin/compare-scenarios")
async def compare_what_if_scenarios(request: Dict[str, Any]):
    """
    Compare multiple what-if scenarios
    
    Request body:
    {
        "scenario_ids": ["scenario_1", "scenario_2"]
    }
    """
    if not app_state.what_if_analyzer:
        return {
            "available": False,
            "message": "What-If Analyzer not initialized"
        }
    
    try:
        scenario_ids = request.get("scenario_ids", [])
        
        if not scenario_ids:
            raise HTTPException(status_code=400, detail="No scenario IDs provided")
        
        comparison = app_state.what_if_analyzer.compare_scenarios(scenario_ids)
        
        return comparison
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error comparing scenarios: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== KNOWLEDGE GRAPH API ENDPOINTS ====================

# ==================== ANALYTICS API ENDPOINTS ====================

@app.get("/api/analytics/defect-trends")
async def get_defect_trends(
    timerange: str = "24h",
    grouping: str = "hourly"
):
    """
    Get defect occurrence trends over time with real-time data
    
    Query Parameters:
        timerange: Time range (24h, 7d, 30d)
        grouping: Data grouping (hourly, daily)
    """
    try:
        # Try to use real-time data from analytics_storage first
        if app_state.analytics_storage and app_state.analytics_storage.get('defects'):
            realtime_defects = app_state.analytics_storage['defects']
            
            # Group defects by time
            defect_counts = {}
            defect_types = ['crack', 'bubble', 'chip', 'stain', 'cloudiness', 'deformation']
            
            for defect in realtime_defects:
                ts = defect['timestamp']
                defect_type = defect['type']
                
                if ts not in defect_counts:
                    defect_counts[ts] = {dt: 0 for dt in defect_types}
                    defect_counts[ts]['total_defects'] = 0
                    defect_counts[ts]['timestamp'] = ts
                
                if defect_type in defect_types:
                    defect_counts[ts][defect_type] += 1
                    defect_counts[ts]['total_defects'] += 1
            
            data_points = list(defect_counts.values())
            
            if data_points:
                logger.info(f"üìä Using real-time defect data: {len(data_points)} time points")
                return {
                    "timerange": timerange,
                    "grouping": grouping,
                    "data_points": data_points,
                    "total_defects": sum(d["total_defects"] for d in data_points),
                    "data_source": "realtime"
                }
        
        # Fallback to synthetic data generation
        now = datetime.utcnow()
        
        if timerange == "24h":
            hours = 24
            interval_minutes = 60 if grouping == "hourly" else 1440
        elif timerange == "7d":
            hours = 168
            interval_minutes = 360 if grouping == "hourly" else 1440
        else:  # 30d
            hours = 720
            interval_minutes = 1440
        
        data_points = []
        for i in range(hours // (interval_minutes // 60)):
            timestamp = (now - timedelta(hours=hours - i * (interval_minutes // 60))).isoformat() + "Z"
            
            # Generate realistic defect counts with daily patterns
            hour_of_day = (hours - i * (interval_minutes // 60)) % 24
            base_count = 5 + np.random.poisson(3)
            # Higher defects during night shifts
            if 22 <= hour_of_day or hour_of_day <= 6:
                base_count = int(base_count * 1.4)
            
            data_points.append({
                "timestamp": timestamp,
                "total_defects": base_count,
                "crack": int(base_count * 0.25),
                "bubble": int(base_count * 0.30),
                "chip": int(base_count * 0.15),
                "stain": int(base_count * 0.15),
                "cloudiness": int(base_count * 0.10),
                "deformation": int(base_count * 0.05)
            })
        
        return {
            "timerange": timerange,
            "grouping": grouping,
            "data_points": data_points,
            "total_defects": sum(d["total_defects"] for d in data_points),
            "data_source": "synthetic"
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting defect trends: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/parameter-correlations")
async def get_parameter_correlations():
    """
    Get correlation matrix between parameters and defects
    """
    try:
        # Realistic correlation coefficients based on physics
        correlations = {
            "furnace_temperature": {
                "crack": 0.78,
                "bubble": -0.65,
                "chip": 0.45,
                "stain": 0.62,
                "cloudiness": 0.52,
                "deformation": 0.71
            },
            "belt_speed": {
                "crack": 0.61,
                "bubble": 0.28,
                "chip": 0.73,
                "stain": -0.35,
                "cloudiness": 0.18,
                "deformation": 0.68
            },
            "mold_temperature": {
                "crack": 0.54,
                "bubble": -0.41,
                "chip": 0.32,
                "stain": 0.28,
                "cloudiness": 0.45,
                "deformation": 0.59
            },
            "forming_pressure": {
                "crack": -0.72,
                "bubble": 0.58,
                "chip": 0.65,
                "stain": 0.22,
                "cloudiness": 0.38,
                "deformation": 0.77
            },
            "cooling_rate": {
                "crack": 0.68,
                "bubble": 0.15,
                "chip": 0.42,
                "stain": 0.31,
                "cloudiness": 0.55,
                "deformation": 0.64
            }
        }
        
        return {
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "correlations": correlations,
            "sample_size": 10000,
            "confidence_level": 0.95
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting parameter correlations: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/analytics/production-efficiency")
async def get_production_efficiency(timerange: str = "24h"):
    """
    Get production rate vs quality rate data points with real-time data
    
    Query Parameters:
        timerange: Time range (24h, 7d, 30d)
    """
    try:
        # Try to use real-time quality metrics first
        if app_state.analytics_storage and app_state.analytics_storage.get('quality_metrics'):
            quality_data = app_state.analytics_storage['quality_metrics']
            
            if quality_data:
                data_points = []
                for metric in quality_data:
                    data_points.append({
                        "timestamp": metric['timestamp'],
                        "production_rate": round(metric.get('production_rate', 150), 1),
                        "quality_rate": round(metric.get('quality_score', 95), 2),
                        "efficiency_score": round(
                            (metric.get('production_rate', 150) / 150) * 
                            (metric.get('quality_score', 95) / 95), 
                            2
                        )
                    })
                
                if data_points:
                    logger.info(f"üìä Using real-time quality metrics: {len(data_points)} points")
                    return {
                        "timerange": timerange,
                        "data_points": data_points,
                        "average_production_rate": round(np.mean([d["production_rate"] for d in data_points]), 1),
                        "average_quality_rate": round(np.mean([d["quality_rate"] for d in data_points]), 2),
                        "optimal_balance": {
                            "production_rate": 150,
                            "quality_rate": 96.5,
                            "efficiency_score": 1.02
                        },
                        "data_source": "realtime"
                    }
        
        # Fallback to synthetic efficiency data
        if timerange == "24h":
            num_points = 24
        elif timerange == "7d":
            num_points = 168
        else:  # 30d
            num_points = 720
        
        data_points = []
        for i in range(num_points):
            # Realistic trade-off: higher production rate -> lower quality
            production_rate = 140 + np.random.randn() * 20
            quality_rate = 98.5 - (production_rate - 150) * 0.08 + np.random.randn() * 1.5
            quality_rate = np.clip(quality_rate, 85, 99.5)
            
            timestamp = (datetime.utcnow() - timedelta(hours=num_points - i)).isoformat() + "Z"
            
            data_points.append({
                "timestamp": timestamp,
                "production_rate": round(production_rate, 1),
                "quality_rate": round(quality_rate, 2),
                "efficiency_score": round((production_rate / 150) * (quality_rate / 95), 2)
            })
        
        return {
            "timerange": timerange,
            "data_points": data_points,
            "average_production_rate": round(np.mean([d["production_rate"] for d in data_points]), 1),
            "average_quality_rate": round(np.mean([d["quality_rate"] for d in data_points]), 2),
            "optimal_balance": {
                "production_rate": 150,
                "quality_rate": 96.5,
                "efficiency_score": 1.02
            },
            "data_source": "synthetic"
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting production efficiency: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== EXPLAINABILITY API ENDPOINTS ====================

@app.get("/api/explainability/feature-importance")
async def get_feature_importance(model_type: str = "lstm"):
    """
    Get feature importance for ML models
    
    Query Parameters:
        model_type: Type of model (lstm, gnn, vit, ensemble)
    """
    try:
        # Generate realistic SHAP-like feature importance values
        features = {
            "lstm": [
                {"feature": "furnace_temperature", "importance": 0.28, "category": "thermal"},
                {"feature": "furnace_temperature_lag_1h", "importance": 0.22, "category": "thermal"},
                {"feature": "belt_speed", "importance": 0.18, "category": "mechanical"},
                {"feature": "forming_pressure", "importance": 0.15, "category": "mechanical"},
                {"feature": "mold_temperature", "importance": 0.12, "category": "thermal"},
                {"feature": "cooling_rate", "importance": 0.10, "category": "thermal"},
                {"feature": "furnace_pressure", "importance": 0.08, "category": "mechanical"},
                {"feature": "belt_speed_variance", "importance": 0.06, "category": "mechanical"},
                {"feature": "temperature_gradient", "importance": 0.05, "category": "thermal"},
                {"feature": "production_hour", "importance": 0.03, "category": "temporal"}
            ],
            "gnn": [
                {"feature": "sensor_network_connectivity", "importance": 0.32, "category": "network"},
                {"feature": "furnace_temperature", "importance": 0.24, "category": "thermal"},
                {"feature": "cross_zone_correlation", "importance": 0.20, "category": "network"},
                {"feature": "forming_pressure", "importance": 0.18, "category": "mechanical"},
                {"feature": "sensor_anomaly_score", "importance": 0.15, "category": "network"},
                {"feature": "belt_speed", "importance": 0.12, "category": "mechanical"}
            ],
            "vit": [
                {"feature": "surface_texture_variance", "importance": 0.35, "category": "visual"},
                {"feature": "edge_sharpness", "importance": 0.28, "category": "visual"},
                {"feature": "color_uniformity", "importance": 0.22, "category": "visual"},
                {"feature": "bubble_pattern", "importance": 0.18, "category": "visual"},
                {"feature": "crack_probability_map", "importance": 0.14, "category": "visual"}
            ]
        }
        
        selected_features = features.get(model_type, features["lstm"])
        
        return {
            "model_type": model_type,
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "features": selected_features,
            "method": "SHAP (SHapley Additive exPlanations)",
            "baseline_score": 0.87
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting feature importance: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/explainability/shap-values")
async def get_shap_values(prediction_id: Optional[str] = None):
    """
    Get SHAP values for a specific prediction
    
    Query Parameters:
        prediction_id: ID of prediction to explain (optional, uses latest if not provided)
    """
    try:
        # Generate synthetic SHAP values for demonstration
        shap_data = {
            "prediction_id": prediction_id or f"pred_{int(datetime.utcnow().timestamp())}",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "base_value": 0.15,  # Base defect probability
            "predicted_value": 0.78,  # Actual prediction
            "shap_values": [
                {"feature": "furnace_temperature", "value": 1620, "shap_value": +0.42},
                {"feature": "furnace_temperature_lag_1h", "value": 1580, "shap_value": +0.18},
                {"feature": "belt_speed", "value": 175, "shap_value": +0.12},
                {"feature": "forming_pressure", "value": 42, "shap_value": -0.08},
                {"feature": "mold_temperature", "value": 335, "shap_value": +0.06},
                {"feature": "cooling_rate", "value": 4.2, "shap_value": +0.04},
                {"feature": "furnace_pressure", "value": 14.5, "shap_value": -0.02}
            ],
            "force_plot_data": {
                "pushing_higher": ["furnace_temperature", "belt_speed", "mold_temperature"],
                "pushing_lower": ["forming_pressure", "furnace_pressure"]
            }
        }
        
        return shap_data
    except Exception as e:
        logger.error(f"‚ùå Error getting SHAP values: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== KNOWLEDGE GRAPH API ENDPOINTS ====================

@app.get("/api/knowledge-graph/root-cause")
async def get_root_cause_analysis(defect_type: str, min_confidence: float = 0.5):
    """
    Get root cause analysis for a defect type
    
    Query Parameters:
        defect_type: Type of defect (crack, bubble, chip, cloudiness, deformation, stain)
        min_confidence: Minimum confidence threshold (0-1)
    """
    try:
        # Get knowledge graph from system integrator
        if (hasattr(app_state.system_integrator, 'system_integrator') and 
            app_state.system_integrator.system_integrator and
            hasattr(app_state.system_integrator.system_integrator, 'knowledge_graph')):
            
            kg = app_state.system_integrator.system_integrator.knowledge_graph
            
            if not kg:
                return {
                    "available": False,
                    "message": "Knowledge Graph not initialized"
                }
            
            # Get current sensor data from data generator
            current_reading = app_state.data_generator.generate_sensor_reading("FURNACE_01_TEMP")
            
            # Extract current parameters
            current_parameters = {
                'furnace_temperature': current_reading.get('value', 1500),
                'furnace_pressure': 15.0 + np.random.randn() * 2,
                'belt_speed': 150.0 + np.random.randn() * 10,
                'mold_temperature': 320.0 + np.random.randn() * 15,
                'forming_pressure': 50.0 + np.random.randn() * 5,
                'cooling_rate': 3.5 + np.random.randn() * 0.5
            }
            
            # Analyze root cause
            root_causes = analyze_root_cause(
                kg, 
                defect_type, 
                current_parameters,
                min_confidence
            )
            
            return {
                "defect_type": defect_type,
                "timestamp": datetime.utcnow().isoformat(),
                "root_causes": root_causes,
                "current_parameters": current_parameters
            }
        else:
            return {
                "available": False,
                "message": "Knowledge Graph not available"
            }
            
    except Exception as e:
        logger.error(f"‚ùå Error getting root cause analysis: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/knowledge-graph/intervention")
async def get_intervention_recommendation(defect_type: str):
    """
    Get intervention recommendations for a defect type
    
    Query Parameters:
        defect_type: Type of defect
    """
    try:
        # Specific recommendations based on defect type
        specific_interventions = {
            'crack': {
                'action': 'Reduce furnace temperature by 20-30¬∞C',
                'outcome': 'Defect rate -40%',
                'confidence': 0.85
            },
            'bubble': {
                'action': 'Decrease forming pressure by 10 MPa',
                'outcome': 'Defect rate -60%',
                'confidence': 0.88
            },
            'deformation': {
                'action': 'Reduce belt speed by 15%',
                'outcome': 'Defect rate -50%',
                'confidence': 0.82
            },
            'cloudiness': {
                'action': 'Increase furnace temperature by 30-50¬∞C',
                'outcome': 'Defect rate -35%',
                'confidence': 0.78
            }
        }
        
        if defect_type in specific_interventions:
            spec = specific_interventions[defect_type]
            interventions = [{
                "defect_type": defect_type,
                "recommended_action": spec['action'],
                "expected_outcome": spec['outcome'],
                "confidence": spec['confidence'],
                "implementation_time": "1-30 minutes"
            }]
        else:
            interventions = [{
                "defect_type": defect_type,
                "recommended_action": f"Adjust parameters for {defect_type} mitigation",
                "expected_outcome": "Defect rate reduction",
                "confidence": 0.75,
                "implementation_time": "5-20 minutes"
            }]
        
        return {
            "defect_type": defect_type,
            "interventions": interventions,
            "timestamp": datetime.utcnow().isoformat()
        }
            
    except Exception as e:
        logger.error(f"‚ùå Error getting intervention recommendations: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== HELPER FUNCTIONS ====================

def generate_mock_predictions(horizon_hours: int) -> List[Dict]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è mock –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
    predictions = []
    
    for hour in range(1, horizon_hours + 1):
        pred_time = datetime.utcnow() + timedelta(hours=hour)
        # Generate more realistic defect probabilities based on typical glass production patterns
        base_probability = 0.15 + 0.1 * np.sin(hour * 0.5)  # Cyclical pattern
        defect_probability = float(np.clip(base_probability + np.random.normal(0, 0.05), 0.05, 0.5))
        
        predictions.append({
            "timestamp": pred_time.isoformat(),
            "defect_probability": defect_probability,
            "expected_defect_count": int(defect_probability * 20 + np.random.randint(1, 4)),
            "risk_level": "HIGH" if defect_probability > 0.35 else ("MEDIUM" if defect_probability > 0.2 else "LOW"),
            "contributing_factors": [
                "temperature_variance" if defect_probability > 0.25 else "speed_fluctuation",
                "pressure_changes" if defect_probability > 0.3 else "humidity_fluctuation"
            ]
        })
    
    return predictions


def generate_recommendations(predictions: List[Dict]) -> List[str]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
    recommendations = []
    
    for pred in predictions[:3]:
        defect_prob = pred.get("defect_probability", 0)
        factors = pred.get("contributing_factors", [])
        
        if defect_prob > 0.35:
            if "temperature_variance" in factors:
                recommendations.append("–£–º–µ–Ω—å—à–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–µ—á–∏ –Ω–∞ 20¬∞C")
                recommendations.append("–°—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–∞—á—É —Ç–æ–ø–ª–∏–≤–∞")
            if "speed_fluctuation" in factors:
                recommendations.append("–°–Ω–∏–∑–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å —Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—è –Ω–∞ 10%")
                recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø—Ä–∏–≤–æ–¥ –∫–æ–Ω–≤–µ–π–µ—Ä–∞")
        elif defect_prob > 0.25:
            recommendations.append("–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –¥–∞–≤–ª–µ–Ω–∏—è —Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—è")
            recommendations.append("–ö–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å –≤–ª–∞–∂–Ω–æ—Å—Ç—å –æ–∫—Ä—É–∂–∞—é—â–µ–π —Å—Ä–µ–¥—ã")
        else:
            recommendations.append("–ü—Ä–æ—Ü–µ—Å—Å —Å—Ç–∞–±–∏–ª–µ–Ω, –∏–∑–º–µ–Ω–µ–Ω–∏–π –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è")
            recommendations.append("–ü—Ä–æ–¥–æ–ª–∂–∞—Ç—å –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤")
    
    return list(set(recommendations))  # Remove duplicates


def generate_mock_alerts() -> List[AlertModel]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è mock –∞–ª–µ—Ä—Ç–æ–≤"""
    return [
        AlertModel(
            alert_id="ALT_001",
            timestamp=(datetime.utcnow() - timedelta(minutes=15)).isoformat(),
            priority="HIGH",
            alert_type="temperature_spike",
            message="–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –ø–µ—á–∏ –ø—Ä–µ–≤—ã—à–∞–µ—Ç –Ω–æ—Ä–º—É –Ω–∞ 45¬∞C",
            affected_sensors=["furnace_temp_01"],
            recommended_actions=["–£–º–µ–Ω—å—à–∏—Ç—å –º–æ—â–Ω–æ—Å—Ç—å –≥–æ—Ä–µ–ª–∫–∏", "–ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–∏—Å—Ç–µ–º—É –æ—Ö–ª–∞–∂–¥–µ–Ω–∏—è"]
        ),
        AlertModel(
            alert_id="ALT_002",
            timestamp=(datetime.utcnow() - timedelta(minutes=8)).isoformat(),
            priority="MEDIUM",
            alert_type="quality_degradation",
            message="–°–Ω–∏–∂–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –ø—Ä–æ–¥—É–∫—Ü–∏–∏ –Ω–∞ 5% –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–π —á–∞—Å",
            affected_sensors=["quality_sensor_01"],
            recommended_actions=["–ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–æ—Ä–º–æ–≤–∞–Ω–∏—è", "–ö–∞–ª–∏–±—Ä–æ–≤–∞—Ç—å –¥–∞—Ç—á–∏–∫–∏"]
        )
    ]


def generate_mock_recommendations() -> List[RecommendationModel]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è mock —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"""
    return [
        RecommendationModel(
            recommendation_id="REC_001",
            timestamp=datetime.utcnow().isoformat(),
            action_type="temperature_adjustment",
            description="–†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è —Å–Ω–∏–∑–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–µ—á–∏ –¥–æ 1520¬∞C",
            urgency="HIGH",
            expected_impact="–°–Ω–∏–∂–µ–Ω–∏–µ —Ä–∏—Å–∫–∞ —Ç—Ä–µ—â–∏–Ω –Ω–∞ 35%",
            confidence=0.89
        )
    ]


# ==================== NEW ENDPOINTS - PHASES 5-8 ====================

@app.get("/api/explainability/prediction")
async def get_prediction_explanation(model_name: str = "lstm"):
    """
    Get explainability for the latest prediction
    Phase 6: Model Explainability Integration
    """
    try:
        explanations = None
        
        # Try to get explanations from pipeline orchestrator
        if app_state.pipeline_orchestrator:
            explanations = app_state.pipeline_orchestrator.latest_explanations
        
        # If no explanations available, generate realistic mock data
        if not explanations or not explanations.get("top_features"):
            # Generate SHAP-like feature importance for demonstration
            model_features = {
                "lstm": [
                    {"feature_name": "furnace_temperature", "importance": 0.42, "value": 1520},
                    {"feature_name": "furnace_temperature_lag_1h", "importance": 0.28, "value": 1515},
                    {"feature_name": "belt_speed", "importance": 0.22, "value": 155},
                    {"feature_name": "forming_pressure", "importance": -0.18, "value": 48},
                    {"feature_name": "mold_temperature", "importance": 0.15, "value": 325},
                    {"feature_name": "cooling_rate", "importance": 0.12, "value": 3.2},
                    {"feature_name": "annealing_temp", "importance": -0.10, "value": 580},
                    {"feature_name": "furnace_pressure", "importance": 0.08, "value": 15.2},
                    {"feature_name": "humidity", "importance": -0.05, "value": 45},
                    {"feature_name": "production_hour", "importance": 0.03, "value": 14},
                ],
                "gnn": [
                    {"feature_name": "sensor_network_connectivity", "importance": 0.38, "value": 0.92},
                    {"feature_name": "cross_zone_correlation", "importance": 0.32, "value": 0.85},
                    {"feature_name": "furnace_temperature", "importance": 0.28, "value": 1520},
                    {"feature_name": "anomaly_score", "importance": 0.22, "value": 0.15},
                    {"feature_name": "forming_pressure", "importance": -0.18, "value": 48},
                    {"feature_name": "belt_speed", "importance": 0.15, "value": 155},
                ],
                "ensemble": [
                    {"feature_name": "furnace_temperature", "importance": 0.35, "value": 1520},
                    {"feature_name": "belt_speed", "importance": 0.25, "value": 155},
                    {"feature_name": "forming_pressure", "importance": 0.20, "value": 48},
                    {"feature_name": "mold_temperature", "importance": 0.15, "value": 325},
                    {"feature_name": "cooling_rate", "importance": 0.12, "value": 3.2},
                ]
            }
            
            features = model_features.get(model_name.lower(), model_features["lstm"])
            explanations = {
                "top_features": features,
                "shap_values": features,
                "base_value": 0.15,
                "predicted_value": 0.68,
                "method": "SHAP (SHapley Additive exPlanations)"
            }
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "model_name": model_name,
            "explanations": explanations
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting prediction explanation: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/pipeline/metrics")
async def get_pipeline_metrics():
    """
    Get pipeline performance metrics
    Phase 7: System Metrics and Monitoring
    """
    try:
        metrics = None
        
        # Try to get metrics from pipeline orchestrator
        if app_state.pipeline_orchestrator:
            try:
                metrics = app_state.pipeline_orchestrator.get_pipeline_metrics()
            except Exception as e:
                logger.debug(f"Pipeline metrics not available: {e}")
        
        # If no metrics available, generate realistic mock data
        if not metrics:
            # Calculate uptime in seconds
            uptime_start = datetime.fromisoformat(app_state.metrics["uptime_start"])
            uptime_seconds = (datetime.utcnow() - uptime_start).total_seconds()
            
            metrics = {
                "pipeline_executions": int(uptime_seconds / 5),  # ~1 execution per 5 seconds
                "successful_predictions": int(uptime_seconds / 5 * 0.97),  # 97% success rate
                "failed_predictions": int(uptime_seconds / 5 * 0.03),
                "avg_latency_ms": 45.5 + np.random.randn() * 5,
                "feature_extraction_time_ms": 12.3 + np.random.randn() * 2,
                "prediction_time_ms": 28.4 + np.random.randn() * 3,
                "explanation_time_ms": 8.2 + np.random.randn() * 1
            }
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "pipeline_metrics": metrics,
            "system_metrics": app_state.metrics
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting pipeline metrics: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/features/latest")
async def get_latest_features():
    """
    Get latest engineered features
    Shows feature engineering pipeline output
    """
    try:
        if not app_state.pipeline_orchestrator:
            raise HTTPException(status_code=503, detail="Pipeline orchestrator not initialized")
        
        features = app_state.pipeline_orchestrator.latest_features
        
        if not features:
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "status": "no_features",
                "message": "No recent sensor data processed"
            }
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "features": features
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting latest features: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/autonomy/status")
async def get_autonomy_status():
    """
    Get autonomous action decision status
    Phase 5: RL Agent Autonomy
    """
    try:
        if not app_state.pipeline_orchestrator:
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "autonomy_enabled": False,
                "reason": "Pipeline orchestrator not initialized"
            }
        
        # Get latest autonomous actions from pipeline
        # This would normally come from the latest pipeline execution
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "autonomy_enabled": True,
            "autonomous_actions_count": 0,
            "approval_required_count": 0,
            "safety_checks_enabled": True,
            "message": "Autonomous action system operational"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting autonomy status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/pipeline/process")
async def process_through_pipeline(sensor_data: Dict[str, Any]):
    """
    Process sensor data through complete end-to-end pipeline
    Demonstrates full integration: Data ‚Üí Features ‚Üí ML ‚Üí Explainability ‚Üí RL ‚Üí Actions
    """
    try:
        if not app_state.pipeline_orchestrator:
            raise HTTPException(status_code=503, detail="Pipeline orchestrator not initialized")
        
        # Process through complete pipeline
        pipeline_output = await app_state.pipeline_orchestrator.process_sensor_data(sensor_data)
        
        # Broadcast results to WebSocket clients
        await app_state.ws_manager.broadcast({
            "type": "pipeline_result",
            "data": pipeline_output
        })
        
        return pipeline_output
        
    except Exception as e:
        logger.error(f"‚ùå Error processing through pipeline: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/training/status")
async def get_training_status():
    """
    Get continuous learning training status
    Phase 8: Continuous Learning
    """
    try:
        if not app_state.pipeline_orchestrator or not app_state.pipeline_orchestrator.continuous_learning:
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "continuous_learning_enabled": False,
                "reason": "Continuous learning not initialized"
            }
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "continuous_learning_enabled": True,
            "learning_cycles_completed": 0,
            "experience_buffer_size": len(app_state.pipeline_orchestrator.feature_buffer),
            "status": "operational"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting training status: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/knowledge-graph/populate")
async def populate_knowledge_graph():
    """
    Populate Knowledge Graph with synthetic production data.
    Creates realistic causal relationships between parameters and defects.
    """
    try:
        if not hasattr(app_state.system_integrator, 'knowledge_graph') or not app_state.system_integrator.knowledge_graph:
            # Try to initialize KG if not available
            from knowledge_graph.causal_graph import EnhancedGlassProductionKnowledgeGraph
            import os
            
            is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER')
            if is_docker:
                neo4j_uri = os.getenv("NEO4J_URI", "bolt://neo4j:7687")
                redis_host = os.getenv("REDIS_HOST", "redis")
            else:
                neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
                redis_host = os.getenv("REDIS_HOST", "localhost")
            
            kg = EnhancedGlassProductionKnowledgeGraph(
                uri=neo4j_uri,
                user=os.getenv("NEO4J_USER", "neo4j"),
                password=os.getenv("NEO4J_PASSWORD", "neo4jpassword"),
                redis_host=redis_host
            )
            result = kg.populate_with_synthetic_data()
            kg.close()
        else:
            result = app_state.system_integrator.knowledge_graph.populate_with_synthetic_data()
        
        logger.info(f"üß† Knowledge Graph populated: {result}")
        return {
            "status": "success",
            "data": result,
            "message": "Knowledge Graph populated with synthetic production data"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error populating Knowledge Graph: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/api/rl/recommendations/detailed")
async def get_rl_recommendations_detailed():
    """
    Get detailed RL recommendations with structured data for dashboard display.
    Returns recommendations in a format suitable for the AdvancedDashboard component.
    """
    try:
        # Get base recommendations from system integrator
        base_recs = await app_state.system_integrator.get_rl_recommendations()
        
        if "error" in base_recs:
            # Generate fallback recommendations for demo
            return {
                "timestamp": datetime.utcnow().isoformat(),
                "recommendations": [
                    {
                        "text": "–°–Ω–∏–∑–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –ø–µ—á–∏ –Ω–∞ 15¬∞C –¥–ª—è –º–∏–Ω–∏–º–∏–∑–∞—Ü–∏–∏ —Ç—Ä–µ—â–∏–Ω",
                        "priority": "high",
                        "impact": 85,
                        "icon": "LocalFireDepartment",
                        "parameter": "furnace_temperature",
                        "current_value": 1550,
                        "suggested_value": 1535,
                        "confidence": 0.88,
                        "model": "PPO_Agent"
                    },
                    {
                        "text": "–£–≤–µ–ª–∏—á–∏—Ç—å —Å–∫–æ—Ä–æ—Å—Ç—å –ª–µ–Ω—Ç—ã –Ω–∞ 5% –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏",
                        "priority": "medium",
                        "impact": 65,
                        "icon": "Speed",
                        "parameter": "belt_speed",
                        "current_value": 150,
                        "suggested_value": 157.5,
                        "confidence": 0.75,
                        "model": "PPO_Agent"
                    },
                    {
                        "text": "–ù–∞—Å—Ç—Ä–æ–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É —Ñ–æ—Ä–º—ã –Ω–∞ 320¬∞C",
                        "priority": "medium",
                        "impact": 60,
                        "icon": "Thermostat",
                        "parameter": "mold_temp",
                        "current_value": 340,
                        "suggested_value": 320,
                        "confidence": 0.72,
                        "model": "PPO_Agent"
                    },
                    {
                        "text": "–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø–æ–¥–∞—á—É —Å—ã—Ä—å—è –¥–ª—è —Å–Ω–∏–∂–µ–Ω–∏—è –∑–∞—Ç—Ä–∞—Ç",
                        "priority": "high",
                        "impact": 90,
                        "icon": "Factory",
                        "parameter": "energy_consumption",
                        "current_value": 520,
                        "suggested_value": 458,
                        "confidence": 0.82,
                        "model": "PPO_Agent"
                    }
                ],
                "confidence": 0.8,
                "source": "PPO_RL_Agent"
            }
        
        # Convert base recommendations to dashboard-friendly format
        detailed_recs = []
        for i, rec_item in enumerate(base_recs.get("recommendations", [])):
            # Handle both string and dictionary recommendations
            if isinstance(rec_item, dict):
                # Dictionary format - extract text from action field
                rec_text = rec_item.get("action", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è")
            else:
                # String format
                rec_text = rec_item
            
            # Map recommendation text to structured format
            rec = {
                "text": rec_text,
                "priority": "high" if "—É–≤–µ–ª–∏—á–∏—Ç—å" in rec_text.lower() or "—É–º–µ–Ω—å—à–∏—Ç—å" in rec_text.lower() else "medium",
                "impact": int(base_recs.get("confidence", 0.7) * 100),
                "icon": "Psychology",
                "confidence": base_recs.get("confidence", 0.7),
                "model": "PPO_Agent"
            }
            
            # Try to determine parameter from text
            if "–º–æ—â–Ω–æ—Å—Ç—å –ø–µ—á–∏" in rec_text.lower() or "—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä" in rec_text.lower():
                rec["icon"] = "LocalFireDepartment"
                rec["parameter"] = "furnace_temperature"
            elif "—Å–∫–æ—Ä–æ—Å—Ç—å" in rec_text.lower() or "–∫–æ–Ω–≤–µ–π–µ—Ä" in rec_text.lower():
                rec["icon"] = "Speed"
                rec["parameter"] = "belt_speed"
            elif "—Ñ–æ—Ä–º" in rec_text.lower():
                rec["icon"] = "Thermostat"
                rec["parameter"] = "mold_temp"
            elif "–≥–æ—Ä–µ–ª" in rec_text.lower():
                rec["icon"] = "LocalFireDepartment"
                rec["parameter"] = "burner_zone"
            
            detailed_recs.append(rec)
        
        return {
            "timestamp": datetime.utcnow().isoformat(),
            "recommendations": detailed_recs,
            "confidence": base_recs.get("confidence", 0.7),
            "source": "PPO_RL_Agent"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Error getting detailed RL recommendations: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# ==================== STARTUP EVENT ====================

# ==================== NOTIFICATION SYSTEM ====================

class NotificationManager:
    """Manages system notifications for critical alerts and warnings"""
    
    def __init__(self):
        self.notifications: List[Dict[str, Any]] = []
        self._notification_counter = 0
        
    def create_notification(
        self,
        category: str,
        priority: str,
        title: str,
        message: str,
        source: str = "SYSTEM",
        actions: Optional[List[Dict[str, Any]]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """Create a new notification"""
        self._notification_counter += 1
        notification = {
            "id": f"notif_{self._notification_counter}_{int(datetime.utcnow().timestamp() * 1000)}",
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "category": category,
            "priority": priority,
            "title": title,
            "message": message,
            "source": source,
            "actions": actions or [],
            "metadata": metadata or {},
            "acknowledged": False,
            "resolved": False,
            "resolution_notes": None
        }
        self.notifications.append(notification)
        
        # Keep only last 1000 notifications to prevent memory issues
        if len(self.notifications) > 1000:
            self.notifications = self.notifications[-1000:]
        
        return notification
    
    def get_active_notifications(
        self,
        priority_filter: Optional[str] = None,
        category_filter: Optional[str] = None
    ) -> List[Dict[str, Any]]:
        """Get active (non-resolved) notifications with optional filtering"""
        filtered = [n for n in self.notifications if not n.get("resolved", False)]
        
        if priority_filter:
            filtered = [n for n in filtered if n["priority"] == priority_filter]
        
        if category_filter:
            filtered = [n for n in filtered if n["category"] == category_filter]
        
        # Sort by priority (CRITICAL > HIGH > MEDIUM > LOW) and then by timestamp
        priority_order = {"CRITICAL": 0, "HIGH": 1, "MEDIUM": 2, "LOW": 3}
        filtered.sort(
            key=lambda x: (priority_order.get(x["priority"], 99), x["timestamp"]),
            reverse=True
        )
        
        # Convert numpy types to native Python types for JSON serialization
        converted_filtered = [self._convert_numpy_types(notification) for notification in filtered]
        
        return converted_filtered    
    
    def _convert_numpy_types(self, obj):
        """
        Recursively convert numpy types to native Python types for JSON serialization
        """
        import numpy as np
        
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.bool_):
            return bool(obj)
        elif isinstance(obj, dict):
            return {key: self._convert_numpy_types(value) for key, value in obj.items()}
        elif isinstance(obj, (list, tuple)):
            return [self._convert_numpy_types(item) for item in obj]
        elif isinstance(obj, (np.datetime64, np.timedelta64)):
            return str(obj)
        else:
            return obj
    
    def acknowledge_notification(self, notification_id: str) -> bool:
        """Mark notification as acknowledged"""
        for notification in self.notifications:
            if notification["id"] == notification_id:
                notification["acknowledged"] = True
                return True
        return False    
    def delete_notification(self, notification_id: str) -> bool:
        """Delete a notification"""
        for i, notification in enumerate(self.notifications):
            if notification["id"] == notification_id:
                del self.notifications[i]
                return True
        return False
    
    def create_defect_alert_notification(
        self,
        defect_type: str,
        severity: str,
        location: str,
        confidence: float
    ):
        """Helper to create defect alert notification"""
        priority_map = {"CRITICAL": "CRITICAL", "HIGH": "HIGH", "MEDIUM": "MEDIUM", "LOW": "LOW"}
        
        return self.create_notification(
            category="CRITICAL_DEFECT",
            priority=priority_map.get(severity, "MEDIUM"),
            title=f"{defect_type.upper()} –æ–±–Ω–∞—Ä—É–∂–µ–Ω",
            message=f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥–µ—Ñ–µ–∫—Ç —Ç–∏–ø–∞ {defect_type} –≤ {location} —Å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é {confidence*100:.1f}%",
            source="ML_MODEL",
            actions=[
                {"label": "–ü—Ä–æ—Å–º–æ—Ç—Ä–µ—Ç—å –¥–µ—Ç–∞–ª–∏", "action": "/defects/details", "params": {"type": defect_type}},
                {"label": "–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –ø—Ä–∏—á–∏–Ω—ã", "action": "/knowledge-graph", "params": {"defect": defect_type}}
            ]
        )
    
    def create_parameter_anomaly_notification(
        self,
        parameter_name: str,
        current_value: float,
        expected_value: float,
        deviation: float
    ):
        """Helper to create parameter anomaly notification"""
        priority = "CRITICAL" if abs(deviation) > 50 else "HIGH" if abs(deviation) > 30 else "MEDIUM"
        
        return self.create_notification(
            category="PARAMETER_ANOMALY",
            priority=priority,
            title=f"–ê–Ω–æ–º–∞–ª–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–∞: {parameter_name}",
            message=f"{parameter_name} –æ—Ç–∫–ª–æ–Ω–∏–ª—Å—è –Ω–∞ {deviation:.1f} –æ—Ç –æ–∂–∏–¥–∞–µ–º–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è {expected_value:.1f} (—Ç–µ–∫—É—â–µ–µ: {current_value:.1f})",
            source="SENSOR",
            actions=[
                {"label": "–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä", "action": "/parameters/adjust", "params": {"name": parameter_name}}
            ]
        )

# Initialize notification manager globally
notification_manager = NotificationManager()

@app.get("/api/notifications/active")
async def get_active_notifications(
    priority: Optional[str] = None,
    category: Optional[str] = None
):
    """Get all active (unresolved) notifications with optional filtering"""
    try:
        notifications = notification_manager.get_active_notifications(
            priority_filter=priority,
            category_filter=category
        )
        
        unacknowledged_count = sum(1 for n in notifications if not n.get("acknowledged", False))
        
        return {
            "notifications": notifications,
            "unacknowledged_count": unacknowledged_count,
            "total_count": len(notifications),
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    except Exception as e:
        logger.error(f"‚ùå Error getting notifications: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/api/notifications/{notification_id}/acknowledge")
async def acknowledge_notification(notification_id: str):
    """Mark a notification as acknowledged"""
    try:
        success = notification_manager.acknowledge_notification(notification_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Notification not found")
        
        return {
            "status": "acknowledged",
            "notification_id": notification_id,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error acknowledging notification: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.delete("/api/notifications/{notification_id}")
async def delete_notification(notification_id: str):
    """Delete a notification"""
    try:
        success = notification_manager.delete_notification(notification_id)
        
        if not success:
            raise HTTPException(status_code=404, detail="Notification not found")
        
        return {
            "status": "dismissed",
            "notification_id": notification_id,
            "timestamp": datetime.utcnow().isoformat() + "Z"
        }
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"‚ùå Error deleting notification: {e}")
        raise HTTPException(status_code=500, detail=str(e))


# NOTE: All notifications are now ML-driven from websocket_broadcaster.py
# No hardcoded sample notifications - only real ML predictions create notifications

# ==================== STARTUP EVENT ====================

@app.on_event("startup")
async def startup_event():
    """–°–æ–±—ã—Ç–∏–µ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ"""
    logger.info("üéØ API —Å–µ—Ä–≤–µ—Ä –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ")
    logger.info("ü§ñ All notifications are now ML-driven (LSTM/GNN predictions)")
    
    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–ø–æ–ª–Ω–∏—Ç—å Knowledge Graph —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    try:
        logger.info("üß† Attempting to populate Knowledge Graph with synthetic data...")
        from knowledge_graph.causal_graph import EnhancedGlassProductionKnowledgeGraph
        import os
        
        is_docker = os.path.exists('/.dockerenv') or os.environ.get('DOCKER_CONTAINER')
        if is_docker:
            neo4j_uri = os.getenv("NEO4J_URI", "bolt://neo4j:7687")
            redis_host = os.getenv("REDIS_HOST", "redis")
        else:
            neo4j_uri = os.getenv("NEO4J_URI", "bolt://localhost:7687")
            redis_host = os.getenv("REDIS_HOST", "localhost")
        
        kg = EnhancedGlassProductionKnowledgeGraph(
            uri=neo4j_uri,
            user=os.getenv("NEO4J_USER", "neo4j"),
            password=os.getenv("NEO4J_PASSWORD", "neo4jpassword"),
            redis_host=redis_host
        )
        result = kg.populate_with_synthetic_data()
        kg.close()
        logger.info(f"‚úÖ Knowledge Graph populated: {result}")
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è KG population skipped: {e}")
    
    # –ó–∞–ø—É—Å–∫ —Ñ–æ–Ω–æ–≤—ã—Ö –∑–∞–¥–∞—á
    asyncio.create_task(realtime_update_broadcaster())


if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
